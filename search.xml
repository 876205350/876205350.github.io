<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[16、"order by"是怎么工作的]]></title>
    <url>%2F2019%2F09%2F11%2F16order-by%E6%98%AF%E6%80%8E%E4%B9%88%E5%B7%A5%E4%BD%9C%E7%9A%84%2F</url>
    <content type="text"><![CDATA[在你开发应用的时候，一定会经常碰到需要根据指定的字段排序来显示结果的需求， 一起看一下具体是如何工作的。 假设这个表的部分定义是这样的：123456789CREATE TABLE `t` ( `id` int(11) NOT NULL, `city` varchar(16) NOT NULL, `name` varchar(16) NOT NULL, `age` int(11) NOT NULL, `addr` varchar(128) DEFAULT NULL, PRIMARY KEY (`id`), KEY `city` (`city`)) ENGINE=InnoDB; 这时，你的SQL语句可以这么写：1select city,name,age from t where city = '杭州' order by name limit 1000; 这个语句看上去逻辑很清晰，但是你了解它的执行流程吗？今天，我就和你聊聊这个语句是怎么执行的，以及有什么参数会影响执行的行为。 全字段排序前面我们介绍过索引，所以你现在就很清楚了，为避免全表扫描，我们需要在city字段加上索引。 在city字段上创建索引之后，我们用explain命令来看看这个语句的执行情况。Extra这个字段中的“Using filesort”表示的就是需要排序，MySQL会给每个线程分配一块内存用于排序，称为sort_buffer。 为了说明这个SQL查询语句的执行过程，我们先来看一下city这个索引的示意图。从图中可以看到，满足city=’杭州’条件的行，是从IDX到ID(X+N)的这些记录 通常情况下，这个语句执行流程如下所示 ： 1.初始化sort_buffer，确定放入name、city、age这三个字段； 2.从索引city找到第一个满足city=’杭州’条件的主键id，也就是图中的ID_X； 3.到主键id索引取出整行，取name、city、age三个字段的值，存入sort_buffer中； 4.从索引city取下一个记录的主键id； 5.重复步骤3、4直到city的值不满足查询条件为止，对应的主键id也就是图中的ID_Y； 6.对sort_buffer中的数据按照字段name做快速排序； 7.按照排序结果取前1000行返回给客户端。我们暂且把这个排序过程，称为全字段排序，执行流程的示意图如下所示，下一篇文章中我们还会用到这个排序。图中“按name排序”这个动作，可能在内存中完成，也可能需要使用外部排序，这取决于排序所需的内存和参数sort_buffer_size。sort_buffer_size，就是MySQL为排序开辟的内存（sort_buffer）的大小。如果要排序的数据量小于sort_buffer_size，排序就在内存中完成。但如果排序数据量太大，内存放不下，则不得不利用磁盘临时文件辅助排序。你可以用下面介绍的方法，来确定一个排序语句是否使用了临时文件。1234567891011121314151617/* 打开optimizer_trace，只对本线程有效 */SET optimizer_trace='enabled=on'; /* @a保存Innodb_rows_read的初始值 */select VARIABLE_VALUE into @a from performance_schema.session_status where variable_name = 'Innodb_rows_read';/* 执行语句 */select city, name,age from t where city='杭州' order by name limit 1000; /* 查看 OPTIMIZER_TRACE 输出 */SELECT * FROM `information_schema`.`OPTIMIZER_TRACE`\G/* @b保存Innodb_rows_read的当前值 */select VARIABLE_VALUE into @b from performance_schema.session_status where variable_name = 'Innodb_rows_read';/* 计算Innodb_rows_read差值 */select @b-@a; 这个方法是通过查看 OPTIMIZER_TRACE 的结果来确认的，你可以从 number_of_tmp_files中看到是否使用了临时文件。number_of_tmp_files表示的是，排序过程中使用的临时文件数。你一定奇怪，为什么需要12个文件？内存放不下时，就需要使用外部排序，外部排序一般使用归并排序算法。可以这么简单理解，MySQL将需要排序的数据分成12份，每一份单独排序后存在这些临时文件中。然后把这12个有序文件再合并成一个有序的大文件。 如果sort_buffer_size超过了需要排序的数据量的大小，number_of_tmp_files就是0，表示排序可以直接在内存中完成。 否则就需要放在临时文件中排序。sort_buffer_size越小，需要分成的份数越多，number_of_tmp_files的值就越大。 接下来，我再和你解释一下图4中其他两个值的意思。 我们的示例表中有4000条满足city=’杭州’的记录，所以你可以看到 examined_rows=4000，表示参与排序的行数是4000行。 sort_mode 里面的packed_additional_fields的意思是，排序过程对字符串做了“紧凑”处理。即使name字段的定义是varchar(16)，在排序过程中还是要按照实际长度来分配空间的。 同时，最后一个查询语句select @b-@a 的返回结果是4000，表示整个执行过程只扫描了4000行。 这里需要注意的是，为了避免对结论造成干扰，我把internal_tmp_disk_storage_engine设置成MyISAM。否则，select @b-@a的结果会显示为4001。 这是因为查询OPTIMIZER_TRACE这个表时，需要用到临时表，而internal_tmp_disk_storage_engine的默认值是InnoDB。如果使用的是InnoDB引擎的话，把数据从临时表取出来的时候，会让Innodb_rows_read的值加1。 rowid排序在上面这个算法过程里面，只对原表的数据读了一遍，剩下的操作都是在sort_buffer和临时文件中执行的。但这个算法有一个问题，就是如果查询要返回的字段很多的话，那么sort_buffer里面要放的字段数太多，这样内存里能够同时放下的行数很少，要分成很多个临时文件，排序的性能会很差。 所以如果单行很大，这个方法效率不够好。 那么，如果MySQL认为排序的单行长度太大会怎么做呢？接下来，我来修改一个参数，让MySQL采用另外一种算法。1SET max_length_for_sort_data = 16; max_length_for_sort_data，是MySQL中专门控制用于排序的行数据的长度的一个参数。它的意思是，如果单行的长度超过这个值，MySQL就认为单行太大，要换一个算法。 city、name、age 这三个字段的定义总长度是36，我把max_length_for_sort_data设置为16，我们再来看看计算过程有什么改变。 新的算法放入sort_buffer的字段，只有要排序的列（即name字段）和主键id。 但这时，排序的结果就因为少了city和age字段的值，不能直接返回了，整个执行流程就变成如下所示的样子： 1.初始化sort_buffer，确定放入两个字段，即name和id； 2.从索引city找到第一个满足city=’杭州’条件的主键id，也就是图中的ID_X； 3.到主键id索引取出整行，取name、id这两个字段，存入sort_buffer中； 4.从索引city取下一个记录的主键id； 5.重复步骤3、4直到不满足city=’杭州’条件为止，也就是图中的ID_Y； 6.对sort_buffer中的数据按照字段name进行排序； 7.遍历排序结果，取前1000行，并按照id的值回到原表中取出city、name和age三个字段返回给客户端。这个执行流程的示意图如下，我把它称为rowid排序。对比图3的全字段排序流程图你会发现，rowid排序多访问了一次表t的主键索引，就是步骤7。 需要说明的是，最后的“结果集”是一个逻辑概念，实际上MySQL服务端从排序后的sort_buffer中依次取出id，然后到原表查到city、name和age这三个字段的结果，不需要在服务端再耗费内存存储结果，是直接返回给客户端的。 根据这个说明过程和图示，你可以想一下，这个时候执行select @b-@a，结果会是多少呢？ 现在，我们就来看看结果有什么不同。 首先，图中的examined_rows的值还是4000，表示用于排序的数据是4000行。但是select @b-@a这个语句的值变成5000了。 因为这时候除了排序过程外，在排序完成后，还要根据id去原表取值。由于语句是limit 1000，因此会多读1000行。从OPTIMIZER_TRACE的结果中，你还能看到另外两个信息也变了。 sort_mode变成了，表示参与排序的只有name和id这两个字段。 number_of_tmp_files变成10了，是因为这时候参与排序的行数虽然仍然是4000行，但是每一行都变小了，因此需要排序的总数据量就变小了，需要的临时文件也相应地变少了。全字段排序 VS rowid排序我们来分析一下，从这两个执行流程里，还能得出什么结论。 如果MySQL实在是担心排序内存太小，会影响排序效率，才会采用rowid排序算法，这样排序过程中一次可以排序更多行，但是需要再回到原表去取数据。 如果MySQL认为内存足够大，会优先选择全字段排序，把需要的字段都放到sort_buffer中，这样排序后就会直接从内存里面返回查询结果了，不用再回到原表去取数据。 这也就体现了MySQL的一个设计思想：如果内存够，就要多利用内存，尽量减少磁盘访问。 对于InnoDB表来说，rowid排序会要求回表多造成磁盘读，因此不会被优先选择。 这个结论看上去有点废话的感觉，但是你要记住它，下一篇文章我们就会用到。 看到这里，你就了解了，MySQL做排序是一个成本比较高的操作。那么你会问，是不是所有的order by都需要排序操作呢？如果不排序就能得到正确的结果，那对系统的消耗会小很多，语句的执行时间也会变得更短。 其实，并不是所有的order by语句，都需要排序操作的。从上面分析的执行过程，我们可以看到，MySQL之所以需要生成临时表，并且在临时表上做排序操作，其原因是原来的数据都是无序的。 你可以设想下，如果能够保证从city这个索引上取出来的行，天然就是按照name递增排序的话，是不是就可以不用再排序了呢？ 确实是这样的。 所以，我们可以在这个市民表上创建一个city和name的联合索引，对应的SQL语句是：1alter table t add index city_user(city, name); 作为与city索引的对比，我们来看看这个索引的示意图。在这个索引里面，我们依然可以用树搜索的方式定位到第一个满足city=’杭州’的记录，并且额外确保了，接下来按顺序取“下一条记录”的遍历过程中，只要city的值是杭州，name的值就一定是有序的。这样整个查询过程的流程就变成了： 1.从索引(city,name)找到第一个满足city=’杭州’条件的主键id； 2.到主键id索引取出整行，取name、city、age三个字段的值，作为结果集的一部分直接返回； 3.从索引(city,name)取下一个记录主键id； 4.重复步骤2、3，直到查到第1000条记录，或者是不满足city=’杭州’条件时循环结束。 可以看到，这个查询过程不需要临时表，也不需要排序。接下来，我们用explain的结果来印证一下。 从图中可以看到，Extra字段中没有Using filesort了，也就是不需要排序了。而且由于(city,name)这个联合索引本身有序，所以这个查询也不用把4000行全都读一遍，只要找到满足条件的前1000条记录就可以退出了。也就是说，在我们这个例子里，只需要扫描1000次。 既然说到这里了，我们再往前讨论，这个语句的执行流程有没有可能进一步简化呢？不知道你还记不记得，我在第5篇文章《 深入浅出索引（下）》中，和你介绍的覆盖索引。 这里我们可以再稍微复习一下。覆盖索引是指，索引上的信息足够满足查询请求，不需要再回到主键索引上去取数据。 按照覆盖索引的概念，我们可以再优化一下这个查询语句的执行流程。 针对这个查询，我们可以创建一个city、name和age的联合索引，对应的SQL语句就是：1alter table t add index city_user_age(city, name, age); 这时，对于city字段的值相同的行来说，还是按照name字段的值递增排序的，此时的查询语句也就不再需要排序了。这样整个查询语句的执行流程就变成了： 1.从索引(city,name,age)找到第一个满足city=’杭州’条件的记录，取出其中的city、name和age这三个字段的值，作为结果集的一部分直接返回； 2.从索引(city,name,age)取下一个记录，同样取出这三个字段的值，作为结果集的一部分直接返回； 3.重复执行步骤2，直到查到第1000条记录，或者是不满足city=’杭州’条件时循环结束。 然后，我们再来看看explain的结果。可以看到，Extra字段里面多了“Using index”，表示的就是使用了覆盖索引，性能上会快很多。 当然，这里并不是说要为了每个查询能用上覆盖索引，就要把语句中涉及的字段都建上联合索引，毕竟索引还是有维护代价的。这是一个需要权衡的决定。 小结今天这篇文章，我和你介绍了MySQL里面order by语句的几种算法流程。 在开发系统的时候，你总是不可避免地会使用到order by语句。你心里要清楚每个语句的排序逻辑是怎么实现的，还要能够分析出在最坏情况下，每个语句的执行对系统资源的消耗，这样才能做到下笔如有神，不犯低级错误。 最后，我给你留下一个思考题吧。假设你的表里面已经有了city_name(city, name)这个联合索引，然后你要查杭州和苏州两个城市中所有的市民的姓名，并且按名字排序，显示前100条记录。如果SQL查询语句是这么写的 ：1mysql&gt; select * from t where city in ('杭州',"苏州") order by name limit 100; 那么，这个语句执行的时候会有排序过程吗，为什么？ 如果业务端代码由你来开发，需要实现一个在数据库端不需要排序的方案，你会怎么实现呢？ 进一步地，如果有分页需求，要显示第101页，也就是说语句最后要改成 “limit 10000,100”， 你的实现方法又会是什么呢？ 你可以把你的思考和观点写在留言区里，我会在下一篇文章的末尾和你讨论这个问题。感谢你的收听，也欢迎你把这篇文章分享给更多的朋友一起阅读。]]></content>
      <categories>
        <category>mysql</category>
      </categories>
      <tags>
        <tag>mysql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[10、MySQL为什么有时会选错索引]]></title>
    <url>%2F2019%2F09%2F10%2F10MySQL%E4%B8%BA%E4%BB%80%E4%B9%88%E6%9C%89%E6%97%B6%E4%BC%9A%E9%80%89%E9%94%99%E7%B4%A2%E5%BC%95%2F</url>
    <content type="text"><![CDATA[在MySQL中一张表其实是可以支持多个索引的。但是，你写SQL语句的时候，并没有主动指定使用哪个索引。也就是说，使用哪个索引是由MySQL来确定的。不知道你有没有碰到过这种情况，一条本来可以执行得很快的语句，却由于MySQL选错了索引，而导致执行速度变得很慢？ 一起来看一个例子吧。我们先建一个简单的表，表里有a、b两个字段，并分别建上索引：12345678CREATE TABLE `t` (`id` int(11) NOT NULL,`a` int(11) DEFAULT NULL,`b` int(11) DEFAULT NULL,PRIMARY KEY (`id`),KEY `a` (`a`),KEY `b` (`b`)) ENGINE=InnoDB； 然后，我们往表t中插入10万行记录，取值按整数递增，即：(1,1,1)，(2,2,2)，(3,3,3) 直到(100000,100000,100000)。 我是用存储过程来插入数据的，这里我贴出来方便你复现：123456789101112delimiter ;create procedure idata()begindeclare i int;set i=1;while(i&lt;=100000)doinsert into t values(i, i, i);set i=i+1;end while;end;;delimiter ;call idata(); 接下来，我们分析一条SQL语句：1mysql&gt; select * from t where a between 10000 and 20000; 你一定会说，这个语句还用分析吗，很简单呀，a上有索引，肯定是要使用索引a的。 你说得没错，图1显示的就是使用explain命令看到的这条语句的执行情况。 从图1看上去，这条查询语句的执行也确实符合预期，key这个字段值是’a’，表示优化器选择了索引a。 不过别急，这个案例不会这么简单。在我们已经准备好的包含了10万行数据的表上，我们再做如下操作。这里，session A的操作你已经很熟悉了，它就是开启了一个事务。随后，session B把数据都删除后，又调用了 idata这个存储过程，插入了10万行数据。 这时候，session B的查询语句select * from t where a between 10000 and 20000就不会再选择索引a了。我们可以通过慢查询日志（slow log）来查看一下具体的执行情况。 为了说明优化器选择的结果是否正确，我增加了一个对照，即：使用force index(a)来让优化器强制使用索引a（这部分内容，我还会在这篇文章的后半部分中提到）。下面的三条SQL语句，就是这个实验过程。123set long_query_time=0;select * from t where a between 10000 and 20000; /*Q1*/select * from t force index(a) where a between 10000 and 20000;/*Q2*/ 第一句，是将慢查询日志的阈值设置为0，表示这个线程接下来的语句都会被记录入慢查询日志中； 第二句，Q1是session B原来的查询； 第三句，Q2是加了force index(a)来和session B原来的查询语句执行情况对比。 如图3所示是这三条SQL语句执行完成后的慢查询日志。 可以看到，Q1扫描了10万行，显然是走了全表扫描，执行时间是40毫秒。Q2扫描了10001行，执行了21毫秒。也就是说，我们在没有使用force index的时候，MySQL用错了索引，导致了更长的执行时间。 这个例子对应的是我们平常不断地删除历史数据和新增数据的场景。这时，MySQL竟然会选错索引，是不是有点奇怪呢？今天，我们就从这个奇怪的结果说起吧。 优化器的逻辑在第一篇文章中，我们就提到过，选择索引是优化器的工作。 而优化器选择索引的目的，是找到一个最优的执行方案，并用最小的代价去执行语句。在数据库里面，扫描行数是影响执行代价的因素之一。扫描的行数越少，意味着访问磁盘数据的次数越少，消耗的CPU资源越少。 当然，扫描行数并不是唯一的判断标准，优化器还会结合是否使用临时表、是否排序等因素进行综合判断。 我们这个简单的查询语句并没有涉及到临时表和排序，所以MySQL选错索引肯定是在判断扫描行数的时候出问题了。那么，问题就是：扫描行数是怎么判断的？MySQL在真正开始执行语句之前，并不能精确地知道满足这个条件的记录有多少条，而只能根据统计信息来估算记录数。这个统计信息就是索引的“区分度”。显然，一个索引上不同的值越多，这个索引的区分度就越好。而一个索引上不同的值的个数，我们称之为“基数”（cardinality）。也就是说，这个基数越大，索引的区分度越好。 我们可以使用show index方法，看到一个索引的基数。如图4所示，就是表t的show index 的结果 。虽然这个表的每一行的三个字段值都是一样的，但是在统计信息中，这三个索引的基数值并不同，而且其实都不准确。 那么，MySQL是怎样得到索引的基数的呢？这里，我给你简单介绍一下MySQL采样统计的方法。 为什么要采样统计呢？因为把整张表取出来一行行统计，虽然可以得到精确的结果，但是代价太高了，所以只能选择“采样统计”。采样统计的时候，InnoDB默认会选择N个数据页，统计这些页面上的不同值，得到一个平均值，然后乘以这个索引的页面数，就得到了这个索引的基数。而数据表是会持续更新的，索引统计信息也不会固定不变。所以，当变更的数据行数超过1/M的时候，会自动触发重新做一次索引统计。在MySQL中，有两种存储索引统计的方式，可以通过设置参数innodb_stats_persistent的值来选择： 设置为on的时候，表示统计信息会持久化存储。这时，默认的N是20，M是10。 设置为off的时候，表示统计信息只存储在内存中。这时，默认的N是8，M是16。由于是采样统计，所以不管N是20还是8，这个基数都是很容易不准的。 但，这还不是全部。 你可以从图4中看到，这次的索引统计值（cardinality列）虽然不够精确，但大体上还是差不多的，选错索引一定还有别的原因。其实索引统计只是一个输入，对于一个具体的语句来说，优化器还要判断，执行这个语句本身要扫描多少行。接下来，我们再一起看看优化器预估的，这两个语句的扫描行数是多少。rows这个字段表示的是预计扫描行数。其中，Q1的结果还是符合预期的，rows的值是104620；但是Q2的rows值是37116，偏差就大了。而图1中我们用explain命令看到的rows是只有10001行，是这个偏差误导了优化器的判断。到这里，可能你的第一个疑问不是为什么不准，而是优化器为什么放着扫描37000行的执行计划不用，却选择了扫描行数是100000的执行计划呢？ 这是因为，如果使用索引a，每次从索引a上拿到一个值，都要回到主键索引上查出整行数据，这个代价优化器也要算进去的。 而如果选择扫描10万行，是直接在主键索引上扫描的，没有额外的代价。 优化器会估算这两个选择的代价，从结果看来，优化器认为直接扫描主键索引更快。当然，从执行时间看来，这个选择并不是最优的。使用普通索引需要把回表的代价算进去，在图1执行explain的时候，也考虑了这个策略的代价 ，但图1的选择是对的。也就是说，这个策略并没有问题。 所以冤有头债有主，MySQL选错索引，这件事儿还得归咎到没能准确地判断出扫描行数。至于为什么会得到错误的扫描行数，这个原因就作为课后问题，留给你去分析了。 既然是统计信息不对，那就修正。analyze table t 命令，可以用来重新统计索引信息。我们来看一下执行效果。这回对了。 所以在实践中，如果你发现explain的结果预估的rows值跟实际情况差距比较大，可以采用这个方法来处理。 其实，如果只是索引统计不准确，通过analyze命令可以解决很多问题，但是前面我们说了，优化器可不止是看扫描行数。 依然是基于这个表t，我们看看另外一个语句：1mysql&gt; select * from t where (a between 1 and 1000) and (b between 50000 and 100000) order by b limit 1; 从条件上看，这个查询没有符合条件的记录，因此会返回空集合。 在开始执行这条语句之前，你可以先设想一下，如果你来选择索引，会选择哪一个呢？ 为了便于分析，我们先来看一下a、b这两个索引的结构图。如果使用索引a进行查询，那么就是扫描索引a的前1000个值，然后取到对应的id，再到主键索引上去查出每一行，然后根据字段b来过滤。显然这样需要扫描1000行。如果使用索引b进行查询，那么就是扫描索引b的最后50001个值，与上面的执行过程相同，也是需要回到主键索引上取值再判断，所以需要扫描50001行。所以你一定会想，如果使用索引a的话，执行速度明显会快很多。那么，下面我们就来看看到底是不是这么一回事儿。 图8是执行explain的结果。1mysql&gt; explain select * from t where (a between 1 and 1000) and (b between 50000 and 100000) order by b limit 1; 可以看到，返回结果中key字段显示，这次优化器选择了索引b，而rows字段显示需要扫描的行数是50198。从这个结果中，你可以得到两个结论： 1.扫描行数的估计值依然不准确； 2.这个例子里MySQL又选错了索引。索引选择异常和处理其实大多数时候优化器都能找到正确的索引，但偶尔你还是会碰到我们上面举例的这两种情况：原本可以执行得很快的SQL语句，执行速度却比你预期的慢很多，你应该怎么办呢？ 一种方法是，像我们第一个例子一样，采用force index强行选择一个索引。MySQL会根据词法解析的结果分析出可能可以使用的索引作为候选项，然后在候选列表中依次判断每个索引需要扫描多少行。如果force index指定的索引在候选索引列表中，就直接选择这个索引，不再评估其他索引的执行代价。 我们来看看第二个例子。刚开始分析时，我们认为选择索引a会更好。现在，我们就来看看执行效果：可以看到，原本语句需要执行2.23秒，而当你使用force index(a)的时候，只用了0.05秒，比优化器的选择快了40多倍。 也就是说，优化器没有选择正确的索引，force index起到了“矫正”的作用。 不过很多程序员不喜欢使用force index，一来这么写不优美，二来如果索引改了名字，这个语句也得改，显得很麻烦。而且如果以后迁移到别的数据库的话，这个语法还可能会不兼容。 但其实使用force index最主要的问题还是变更的及时性。因为选错索引的情况还是比较少出现的，所以开发的时候通常不会先写上force index。而是等到线上出现问题的时候，你才会再去修改SQL语句、加上force index。但是修改之后还要测试和发布，对于生产系统来说，这个过程不够敏捷。 所以，数据库的问题最好还是在数据库内部来解决。那么，在数据库里面该怎样解决呢？ 既然优化器放弃了使用索引a，说明a还不够合适，所以第二种方法就是，我们可以考虑修改语句，引导MySQL使用我们期望的索引。比如，在这个例子里，显然把“order by b limit 1” 改成 “order by b,a limit 1” ，语义的逻辑是相同的。我们来看看改之后的效果：之前优化器选择使用索引b，是因为它认为使用索引b可以避免排序（b本身是索引，已经是有序的了，如果选择索引b的话，不需要再做排序，只需要遍历），所以即使扫描行数多，也判定为代价更小。 现在order by b,a 这种写法，要求按照b,a排序，就意味着使用这两个索引都需要排序。因此，扫描行数成了影响决策的主要条件，于是此时优化器选了只需要扫描1000行的索引a。 当然，这种修改并不是通用的优化手段，只是刚好在这个语句里面有limit 1，因此如果有满足条件的记录， order by b limit 1和order by b,a limit 1 都会返回b是最小的那一行，逻辑上一致，才可以这么做。 如果你觉得修改语义这件事儿不太好，这里还有一种改法，图11是执行效果。1mysql&gt; select * from (select * from t where (a between 1 and 1000) and (b between 50000 and 100000) order by b limit 100)alias limit 1; 在这个例子里，我们用limit 100让优化器意识到，使用b索引代价是很高的。其实是我们根据数据特征诱导了一下优化器，也不具备通用性。第三种方法是，在有些场景下，我们可以新建一个更合适的索引，来提供给优化器做选择，或删掉误用的索引。 不过，在这个例子中，我没有找到通过新增索引来改变优化器行为的方法。这种情况其实比较少，尤其是经过DBA索引优化过的库，再碰到这个bug，找到一个更合适的索引一般比较难。 如果我说还有一个方法是删掉索引b，你可能会觉得好笑。但实际上我碰到过两次这样的例子，最终是DBA跟业务开发沟通后，发现这个优化器错误选择的索引其实根本没有必要存在，于是就删掉了这个索引，优化器也就重新选择到了正确的索引。 小结今天我们一起聊了聊索引统计的更新机制，并提到了优化器存在选错索引的可能性。 对于由于索引统计信息不准确导致的问题，你可以用analyze table来解决。 而对于其他优化器误判的情况，你可以在应用端用force index来强行指定索引，也可以通过修改语句来引导优化器，还可以通过增加或者删除索引来绕过这个问题。 你可能会说，今天这篇文章后面的几个例子，怎么都没有展开说明其原理。我要告诉你的是，今天的话题，我们面对的是MySQL的bug，每一个展开都必须深入到一行行代码去量化，实在不是我们在这里应该做的事情。 所以，我把我用过的解决方法跟你分享，希望你在碰到类似情况的时候，能够有一些思路。 你平时在处理MySQL优化器bug的时候有什么别的方法，也发到评论区分享一下吧。 最后，我给你留下一个思考题。前面我们在构造第一个例子的过程中，通过session A的配合，让session B删除数据后又重新插入了一遍数据，然后就发现explain结果中，rows字段从10001变成37000多。 而如果没有session A的配合，只是单独执行delete from t 、call idata()、explain这三句话，会看到rows字段其实还是10000左右。你可以自己验证一下这个结果。 这是什么原因呢？也请你分析一下吧。 你可以把你的分析结论写在留言区里，我会在下一篇文章的末尾和你讨论这个问题。感谢你的收听，也欢迎你把这篇文章分享给更多的朋友一起阅读。 上期问题时间我在上一篇文章最后留给你的问题是，如果某次写入使用了change buffer机制，之后主机异常重启，是否会丢失change buffer和数据。 这个问题的答案是不会丢失，留言区的很多同学都回答对了。虽然是只更新内存，但是在事务提交的时候，我们把change buffer的操作也记录到redo log里了，所以崩溃恢复的时候，change buffer也能找回来。 在评论区有同学问到，merge的过程是否会把数据直接写回磁盘，这是个好问题。这里，我再为你分析一下。 merge的执行流程是这样的： 1.从磁盘读入数据页到内存（老版本的数据页）； 2.从change buffer里找出这个数据页的change buffer 记录(可能有多个），依次应用，得到新版数据页； 3.写redo log。这个redo log包含了数据的变更和change buffer的变更。 到这里merge过程就结束了。这时候，数据页和内存中change buffer对应的磁盘位置都还没有修改，属于脏页，之后各自刷回自己的物理数据，就是另外一个过程了。]]></content>
      <categories>
        <category>mysql</category>
      </categories>
      <tags>
        <tag>mysql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Mysql中MVCC原理讲解]]></title>
    <url>%2F2019%2F08%2F30%2FMysql%E4%B8%ADMVCC%E5%8E%9F%E7%90%86%E8%AE%B2%E8%A7%A3%2F</url>
    <content type="text"><![CDATA[英文全称为Multi-Version Concurrency Control 多版本并发控制。 准备测试环境：MySQL 5.7.23-log数据库默认隔离级别；RR（Repeatable Read，可重复读），MVCC主要适用于MySQL的RC，RR隔离级别创建一张存储引擎为testmvcc的表，sql为:1234CREATE TABLE testmvcc ( id int(11) DEFAULT NULL, name varchar(11) DEFAULT NULL) ENGINE=InnoDB DEFAULT CHARSET=utf8; 什么是MVCC英文全称为Multi-Version Concurrency Control即 多版本并发控制。他无非就是乐观锁的一种实现方式。在Java编程中，如果把乐观锁看成一个接口，MVCC便是这个接口的一个实现类而已。 特点 1.MVCC其实广泛应用于数据库技术，像Oracle,PostgreSQL等也引入了该技术，即适用范围广。 2.MVCC并没有简单的使用数据库的行锁，而是使用了行级锁，row_level_lock,而非InnoDB中的innodb_row_lock.基本原理MVCC的实现，通过保存数据在某个时间点的快照来实现的。这意味着一个事务无论运行多长时间，在同一个事务里能够看到数据一致的视图。根据事务开始的时间不同，同时也意味着在同一个时刻不同事务看到的相同表里的数据可能是不同的。基本特征 每行数据都存在一个版本，每次数据更新时都更新该版本。 修改时Copy出当前版本随意修改，各个事务之间无干扰。 保存时比较版本号，如果成功（commit），则覆盖原记录；失败则放弃copy（rollback）InnoDB存储引擎MVCC的实现策略在每一行数据中额外保存两个隐藏的列：当前行创建时的版本号和删除时的版本号（可能为空，其实还有一列称为回滚指针，用于事务回滚，不在本文范畴）。这里的版本号并不是实际的时间值，而是系统版本号。每开始新的事务，系统版本号都会自动递增。事务开始时刻的系统版本号会作为事务的版本号，用来和查询每行记录的版本号进行比较。 每个事务又有自己的版本号，这样事务内执行CRUD操作时，就通过版本号的比较来达到数据版本控制的目的。 MVCC下InnoDB的增删查改是怎么work的1、插入数据（insert）:记录的版本号即当前事务的版本号执行一条数据语句：1mysql&gt; insert into testmvcc values(1,"test"); 假设事务id为1，那么插入后的数据行如下： id name create version delete version 1 test 1 - 2、在更新操作的时候，采用的是先标记旧的那行记录为已删除，并且删除版本号是事务版本号，然后插入一行新的记录的方式。比如，针对上面那行记录，事务Id为2 要把name字段更新1mysql&gt; update table set name= 'new_value' where id=1; 事务id为2，那么修改id=1后的数据行如下： id name create version delete version 1 test 1 2 1 new_value 2 - 3、删除操作的时候，就把事务版本号作为删除版本号。比如1mysql&gt; delete from table where id=1; 删除id=1数据如下： id name create version delete version 1 test 1 3 4、查询操作：从上面的描述可以看到，在查询时要符合以下两个条件的记录才能被事务查询出来： 1) 删除版本号未指定或者大于当前事务版本号，即查询事务开启后确保读取的行未被删除。(即上述事务id为2的事务查询时，依然能读取到事务id为3所删除的数据行) 2) 创建版本号 小于或者等于 当前事务版本号 ，就是说记录创建是在当前事务中（等于的情况）或者在当前事务启动之前的其他事物进行的insert。 （即事务id为2的事务只能读取到create version&lt;=2的已提交的事务的数据集）123456补充：1.MVCC手段只适用于Msyql隔离级别中的读已提交（Read committed）和可重复读（Repeatable Read）.2.Read uncimmitted由于存在脏读，即能读到未提交事务的数据行，所以不适用MVCC.原因是MVCC的创建版本和删除版本只要在事务提交后才会产生。3.串行化由于是会对所涉及到的表加锁，并非行锁，自然也就不存在行的版本控制问题。4.通过以上总结，可知，MVCC主要作用于事务性的，有行锁控制的数据库模型。 关于Mysql中MVCC的总结客观上，我们认为他就是乐观锁的一整实现方式，就是每行都有版本号，保存时根据版本号决定是否成功。但由于Mysql的写操作会加排他锁（前文有讲），如果锁定了还算不算是MVCC？了解乐观锁的小伙伴们，都知道其主要依靠版本控制，即消除锁定，二者相互矛盾，so从某种意义上来说，Mysql的MVCC并非真正的MVCC，他只是借用MVCC的名号实现了读的非阻塞而已。]]></content>
      <categories>
        <category>mysql</category>
      </categories>
      <tags>
        <tag>mysql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python安装链接mysql数据库]]></title>
    <url>%2F2019%2F08%2F10%2Fpython%E4%BD%BF%E7%94%A8%E9%93%BE%E6%8E%A5mysql%E6%95%B0%E6%8D%AE%E5%BA%93%2F</url>
    <content type="text"><![CDATA[python 如何使用 mysql 以及配置 首先Django 2.2支持Python 3.5,3.6和3.7 python3自带的pip 首先我们要知道python链接mysql需要三方包的，目前主流的方式就是pymysql 和 mysqlclient（也就是Python3版本的MySQLdb）。一、pymysql1）纯Python实现的，安装简单（直接pip安装） 2) 由于纯Python实现的，可以很好的跟gevent框架结合 二、mysqlclient1）是一个C扩展模块,编译安装可能会导致报各种错误,明显没有pymysql方便 2）速度快； 安装使用pymysql12]]></content>
      <categories>
        <category>mysql</category>
      </categories>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[总结]]></title>
    <url>%2F2019%2F06%2F26%2Finterview%2F</url>
    <content type="text"><![CDATA[总结 第一次面试今天面了第一家公司，去了以后填写了申请报表人事看到工作经历没有问了我一句，感觉没什么希望了。公司名称:建培教育简介:小型，培训教育类 讲一下tp3.2中的查询方法如何实现的。 讲一下你对MVC的理解。 php中的接口和抽象类的区别。 CSS中的四种定位。 人事打断后问了一下之前做过哪些项目，然后就说公司要招一些有工作经验的人。 第二次面试公司名称:北京速通网电子商务有限公司简介: 企业年轻人较多有活力 一面:人力资源主管 一个团队中什么最重要 你的优势是什么 你有什么想了解的 二面:技术面 简单做一下自我介绍 在学校参加过哪些课外活动 聊了一下简历上的项目 说出你用过的PHP中操作数组的函数 如何截取一个字符串，获取字符串的长度 Linux下搭建环境，基本命令会吗 项目中有用到redis吗 数据库中数据量过大如何优化查询语句 get和post的区别 大概问了这些问题感觉自己基础薄弱！明明知道面上的希望不大但还是主动问了人力资源主管面试结果，他说和团队不匹配，并不是技术问题。 第三次面试公司名称：百变悟空科技一面：HR 介绍一下公司主要业务 问了一下基本信息，期望薪资。 一道冒泡排序题 二面：技术面 简单做一下自我介绍 数据库优化方面知识myisam和innodb的区别innodb事务数据库的锁复合索引 nosql中的redis数据类型 简单聊了一下简历上的项目 第四次面试公司名称：学知在线教育只有技术面 说一下laravel路由 谈一下使用过的laravel中间件 PHP常用的魔术方法 说一下AOP和OOP 常用的git命令 Vue中的路由参数如何传递 myisam和innodb的区别 讲一下laravel中的ORM 聊了一下项目 逻辑题：N个火柴，没根火柴燃烧完需要1个小时，火柴只能被点燃，怎样得出时间经过45分钟。 第五次面试公司名称：北京果粒科技直接技术： 简单介绍一下自己 说一下用过的PHP数组处理函数 使用过PHP7吗？ 构造方法与析构方法 public，private，protect修饰类的区别 公司分的任务没有在当天按时完成怎么办？ 项目评估一个月完成，现需要半个月完成怎么办？ 给你加派人手你会如何分配项目 聊一下项目，问最近有接触什么新技术]]></content>
      <categories>
        <category>面试</category>
      </categories>
      <tags>
        <tag>面试</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[PHP的strtotime]]></title>
    <url>%2F2018%2F12%2F11%2FPHP%E7%9A%84strtotime%2F</url>
    <content type="text"><![CDATA[经常会有人被strtotime结合-1 month, +1 month, next month的时候搞得很困惑,然后就会觉得这个函数有点不那么靠谱, 动不动就出问题. 用的时候就会很慌 我也遇到过同样的问题，刚好看到鸟哥（laruence）博客本文转自地址: http://www.laruence.com/2018/07/31/3207.html今天是2018-07-31 执行代码:1date("Y-m-d",strtotime("-1 month")) 怎么输出是2018-07-01? 我们来模拟下date内部的对于这种事情的处理逻辑: 先做-1 month, 那么当前是07-31, 减去一以后就是06-31. 再做日期规范化, 因为6月没有31号, 所以就好像2点60等于3点一样, 6月31就等于了7月1 是不是逻辑很”清晰”呢? 我们也可以手动验证第二个步骤, 比如:12var_dump(date("Y-m-d", strtotime("2017-06-31")));//输出2017-07-01 也就是说, 只要涉及到大小月的最后一天, 都可能会有这个迷惑, 我们也可以很轻松的验证类似的其他月份, 印证这个结论: 12345678var_dump(date("Y-m-d", strtotime("-1 month", strtotime("2017-03-31"))));//输出2017-03-03var_dump(date("Y-m-d", strtotime("+1 month", strtotime("2017-08-31"))));//输出2017-10-01var_dump(date("Y-m-d", strtotime("next month", strtotime("2017-01-31"))));//输出2017-03-03var_dump(date("Y-m-d", strtotime("last month", strtotime("2017-03-31"))));//输出2017-03-03 那怎么办呢? 从PHP5.3开始呢, date新增了一系列修正短语, 来明确这个问题, 那就是”first day of” 和 “last day of”, 也就是你可以限定好不要让date自动”规范化”: 12345678var_dump(date("Y-m-d", strtotime("last day of -1 month", strtotime("2017-03-31"))));//输出2017-02-28var_dump(date("Y-m-d", strtotime("first day of +1 month", strtotime("2017-08-31"))));////输出2017-09-01var_dump(date("Y-m-d", strtotime("first day of next month", strtotime("2017-01-31"))));////输出2017-02-01var_dump(date("Y-m-d", strtotime("last day of last month", strtotime("2017-03-31"))));////输出2017-02-28 那如果是5.3之前的版本(还有人用么?), 你可以使用mktime之类的, 把所有的日子忽略掉, 比如都限定为每月1号就可以了, 只不过就不如直接用first day来的更加优雅.]]></content>
      <categories>
        <category>PHP</category>
      </categories>
      <tags>
        <tag>PHP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[PHPstorm+Xdebug安装配置及问题]]></title>
    <url>%2F2018%2F09%2F06%2FPHPstorm-Xdebug%E5%AE%89%E8%A3%85%E9%85%8D%E7%BD%AE%2F</url>
    <content type="text"><![CDATA[Xdebug的安装，phpstorm的Xdebug调试 环境windows操作系统PHP7.1.13phpstorm版本2017.3 Xdebug安装配置1.查看PHP版本对应的Xdebug版本查看页面源代码然后全选粘贴到框里分析查看结果下载.dll2.去xdebug官网下载对应php版本的xdebug放到php ext目录下面：D：\ phpStudy \ php \ php-7.1.13-nts \ ext链接：https://xdebug.org/download.php 3.打开php.ini中添加如下代码1234567891011[XDebug]xdebug.profiler_output_dir=&quot;F:\phpStudy\tmp\xdebug&quot;xdebug.trace_output_dir=&quot;F:\phpStudy\tmp\xdebug&quot;xdebug.remote_enable=1 xdebug.var_display_max_children=128 xdebug.var_display_max_data=51200000 xdebug.var_display_max_depth=5 xdebug.remote_host = &quot;127.0.0.1&quot;xdebug.remote_port = 9000xdebug.idekey = PHPSTORM zend_extension=&quot;F:\phpStudy\php\php-7.0.12-nts\ext\php_xdebug.dll&quot; phpstorm配置指定本地PHP环境File -&gt; settings -&gt; Languages&amp;Frameworks -&gt; PHPFile -&gt; settings -&gt; Languages&amp;Frameworks -&gt; PHP -&gt; DebugFile -&gt; settings -&gt; Languages&amp;Frameworks -&gt; PHP -&gt; Debug -&gt;DBGp Proxy 添加本地虚拟主机域名File -&gt; settings -&gt; Languages&amp;Frameworks -&gt; PHP -&gt;Servers 首先我们需要对浏览器安装Xdebug helper插件，用于在请求中添加参数，类似：XDEBUG_SESSION_START = session_name。更改配置 运行项目Run -&gt; Edit Configurations运行项目之前打开监听打断点查看断点处数据123456789左侧绿色三角形：Resume Program，表示将继续执行，直到下一个中​​断点停止。左侧红色方形：停止，表示中断当前程序调试。上方第一个图形示：Step Over，跳过当前函数。上方第二个图形示：Step Into，进入当前函数内部的程序（相当于观察程序一步一步执行）。上方第三个图形示：强制进入，强制进入当前函数内部的程序。上方第四个图形示：Step Out，跳出当前函数内部的程式。上方第五个图形示：Run to Cursor，定位到当前光标。变量：可以观察到所有全局变量，当前局部变量的数值手表：可以新增变量，观察变量随着程序执行的变化。 遇到一些异常Debug session was finished without being paused这样的报错，路径映射问题。可能是因为请求的是：http://localhost:80/index.php ，而在PHPStorm中的debug servers配置的是127.0.0.1:80，这就导致了域名不相同了，所以只要改过来就可以了。 调试时从index.php第一行开始把前面的对勾去掉就可以了。 断点显示执行，debug控制台无断点信息说一下原因：我用的PHP的xdebug扩展版本是2.7.2，而当前的phpstorm版本为2017.3.4当前phpstorm版本不支持xdebug版本解决方案：1.降低xdebug，（我重新安装xdebug2.6版本）2.安装更高版本的phpstorm 两行警告! The script ‘D:\phpStudy\PHPTutorial\WWW\test.php’ is outside the project.! Click to set up path mappings有的说编辑servers打上对勾我个人的看法是你当前打开的.idea文件夹不包括当前调试的文件。比如：我打开的文件夹manager，调试的test.php不在manager文件夹里然后我更换到WWW目录下进行调试，警告消失问题解决。 …………..]]></content>
      <categories>
        <category>PHP</category>
      </categories>
      <tags>
        <tag>Xdebug</tag>
        <tag>phpstorm</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[git基础命令]]></title>
    <url>%2F2018%2F06%2F14%2Farticle-git%2F</url>
    <content type="text"><![CDATA[总结一下git的命令和个人理解 ##安装gitWindows上安装git可以从git官网下载安装，一直默认即可。在开始菜单中找到Git Bash 点击后会出现命令行窗体（可以看到版本号）安装完成后进行配置，命令行输入:12git config --global user.name "Your Name"git config --global user.email "email@example.com" 因为Git是分布式版本控制系统，所以，每个机器都必须自报家门：你的名字和Email地址。 ##说一下git clonegit clone支持https和git（即ssh）两种方式下载源码: 当使用git方式下载时，如果没有配置过ssh key，则会有如下错误提示:介绍一下如何配置git的ssh key 首先检查一下刚刚的用户名邮箱:1git config --global --list 然后执行以下命令生成秘钥:1ssh-keygen -t rsa -C "你的邮箱" 点击回车需要几步操作: 1.确认秘钥的保存路径（如果不需要改路径则直接回车）；2.如果上一步置顶的保存路径下已经有秘钥文件，则需要确认是否覆盖（如果之前的秘钥不再需要则直接回车覆盖，如需要则手动拷贝到其他目录后再覆盖）；3.创建密码（如果不需要密码则直接回车）；4.确认密码；在指定路径下会有id_rsa和id_rsa.pub两个文件打开你的github点击头像-&gt;settings左侧列表的SSH and GPG keys 点击绿色的New SSH key然后用文本工具打开之前生成的id_rsa.pub文件，把内容拷贝到key下面的输入框，并为这个key定义一个名称（通常用来区分不同主机），然后保存：尝试用git方式下载，可以看到已经可以正常下载: 以上几步完成以后此时你就和git有了一对一联系，上传下载就如同有了令牌一样 git的基本命令git是什么git是一种版本控制器. 更直白说,团队开发时,管理代码用的软件. 说一下安装Window环境:到 https://git-for-windows.github.io/ 下载软件, 双击,一路”Next”,安装完毕. 到开始菜单找”git bash”,如下图Linux环境安装git:12# ubuntu,debian# $ sudo apt-get install git centos,redhat系统1# yum install git 代码管理创建版本库123$ cd E:/ (集成环境进入到www目录下)$ mkdir test$ git init 注意: 不要把仓库建在中文目录下,可能出问题. .git是个隐藏目录,不要乱碰.(你的每一次代码修改它都帮你记录着呢) 添加文件在刚刚创建的test目下创建一个index.php如图：查看仓库状态1$ git status 可见,此时git发现有一个新文件,但并没有把此文件纳入管理. 我们需要两步,让git仓库管理index.phpgit add index.php把index.php提交到暂存区git commit -m “新建index.php”把index.php提交到版本库12$ git add index.php$ git commit -m &quot;新建index.php&quot; 修改文件修改后的文件需要重新提交到暂存区然后到版本库12$ git add index.php$ git commit -m &quot;修改第2行&quot; 删除文件用rm命令删除文件,并直接commit,提交到版本库例:先创建一个foo.php,供练习删除用12345678910111213$ touch foo.php # 创建foo.php $ git add foo.php $ git commit -m &quot;练习删除用&quot; $ ls foo.php index.php# 开始删除 $ git rm foo.php rm &apos;foo.php&apos;$ git commit -m &quot;删除foo.php&quot; [master e4dc37c] 删除foo.php 1 file changed, 0 insertions(+), 0 deletions(-) delete mode 100644 foo.php$ ls index.php 远程仓库经过前面的练习,你在本地的仓库里管理代码已经比较熟练了.但如果是团队开发,如何配合起来呢?我们可以把版本仓库放在互联网上. 开发者把自己最新的版本推到线上仓库, 同时,把线上仓库的最新代码,拉到自己本地. 这样,就可以配合工作了. 注册git在线仓库的账号国外: http://www.github.com国内: http://git.oschina.net 创建项目注册后,”新建项目”,我们先建一个测试项目为此项目提供的仓库地址有2个. http地址:https://github.com/876205350/lianxi.git ssh地址: git@github.com:876205350/lianxi.git前面因为ssh key已经配好使用ssh更方便不过下面我们依然使用http地址 把代码推送到远程仓库为本地库添加远程库$ git remote add origin https://github.com/876205350/lianxi.git意思是:添加1个远程库,代号是origin,地址是 https://....lianxi.gitpush 推代码$git push -u origin master 意思是,把本地的版本(默认是master),推到代号为origin的远程库去.这个过程会弹出github登录框，让你输入用户名/密码以后再使用http就不用输入账号密码 代码管理学习git,你必须要了解3个重要区域.工作区, 即开发者的工作目录.暂存区, 修改已被记录,但尚未录入版本库的区域.版本库, 存储变化日志及版本信息. 工作区-&gt;add-&gt;到暂存区-&gt;commit-&gt;仓库 文件操作添加多个文件git add #添加file1,file2git add *.txt #添加当前目录下的.txt文档git add . #添加当前目录的所有变化 删除文件git rm 移动或改名git mv 源文件 新文件例移动:git mv config.php ./inc/config.php改名:git mv config.php config.inc.php 改动日志每个文件/目录发生的版本变化,我们都可以追溯.命令为:”git log “常用格式: git log 查看项目的日志git log 查看某文件的日志git log . 查看本目录的日志例: git log 显示如下:123456789101112commit 37285a5a9bc5b62609c5e81dacc4daafab1b9600Author: lucy &lt;lucy@xx.com&gt;Date: Thu Nov 12 17:09:04 2015 +0800 new readme.txt ... ...commit c7dfbb8a7ab6c6377040a20c851216572a79d0a0 Author: yanshiba &lt;yanshiba@gmail.com&gt; Date: Mon Nov 9 15:08:05 2015 +0800 新建index.php 如果感觉log有点乱,可以git log –pretty=oneline,让日志单行显示.12337285a5a9bc5b62609c5e81dacc4daafab1b9600 new readme.txt ... c7dfbb8a7ab6c6377040a20c851216572a79d0a0 新建index.php 切换版本git reflog 查看版本变化12345$ git reflog 5d5df85 HEAD@&#123;0&#125;: commit: four 6207e59 HEAD@&#123;1&#125;: commit: three 70110b9 HEAD@&#123;2&#125;: commit: two bc65223 HEAD@&#123;3&#125;: commit (initial): one HEAD指向当前版本5d5df86,切换为head的前1版本,git reset –hard HEAD^切换为head的前2版本,git reset –hard HEAD^^切换为head的前100版本,git reset –hard HEAD~100 实例:12$ git reset --hard HEAD^^ HEAD is now at 70110b9 two 也可以利用版本号来切换,例12$ git reset --hard 6207e59HEAD is now at 6207e59 three 分支管理分支有什么用？在开发中,遇到这样的情况怎么办?网站已有支付宝在线支付功能,要添加”微信支付”.修改了3个文件, wechat.php,pay.php 刚做到一半,突然有个紧急bug: 支付宝支付后不能修改订单状态.你需要立即马上修改这个bug,需要修改的文件是,ali.php,pay.php. 问题是:pay.php,已经被你修改过,而且尚未完成.直接在此基础上改,肯定有问题.把pay.php倒回去? 那我之前的工作白费了. 此时你肯定会想: 在做”微信支付”时,能否把仓库复制一份,在此副本上修改,不影响原仓库的内容.修改完毕后,再把副本上的修改合并过去.好的,这时你已经有了分支的思想. 前面见过的master,即是代码的主干分支,事实上,在实际的开发中,往往不会直接修改和提交到master分支上.而是创建一个dev分支,在dev分支上,修改测试,没问题了,再把dev分支合并到master上. 如果有了分支,刚才的难题就好解决了,如下图:在做”微信支付”时,我们创建一个wechat分支.把wechat分支commit,此时,master分支内容不会变,因为分支不同. 当遇到紧急bug时,创建一个AliBug分支.修复bug后,把AliBug分支合并到master分支上. 再次从容切换到wechat分支上,接着开发”微信支付”功能,开发完毕后,把wechat分支合并到master分支上. 查看分支查看所有分支git branch列12git branch * master # 说明只有master分支,且处于master分支. 创建分支创建dev分支 git branch dev1234git branch dev # 创建dev分支git branch #查看分支 dev * master # dev分支创建成功,但仍处于master分支 切换分支切换到dev分支 git checkout dev再次查看123$ git branch * dev master # 已切换到dev分支上 合并分支当我们在dev上开发某功能,并测试通过后,可以把dev的内容合并到master分支.例:当前的readme.txt 内容为”so so”,在dev分支下,添加一行”from dev”并提交12git add readme.txt git commit -m &quot;mod in dev&quot; 再次切换到master,查看readme.txt的内容,仍为’so so’合并dev分支,git merge dev, 如下:12345$ git merge dev Updating c5364fe..412926b Fast-forward readme.txt | 1 + 1 file changed, 1 insertion(+) 再次查看readme.txt的内容,已变为”soso from dev”; 删除分支12git branch -d dev Deleted branch dev (was 412926b). 快速创建和切换分支 快速创建和切换分支git checkout -b dev # 创建dev分支并立即切换到dev分支即起到git branch dev和git checkout dev的共同作用. 远程仓库查看远程仓库查看远程仓库:git remote查看仓库地址:git remote -v例:123git remote -v origin https://git.oschina.net/lianshou/test.git (fetch) origin https://git.oschina.net/lianshou/test.git (push) 删除远程库命令:git remote remove &lt;远程库名&gt;示例:git remote remove origin 添加远程库命令:git remote add &lt;远程库名&gt; &lt;远程库地址&gt; 示例:1git remote add origin https://github.com/876205350/lianxi.git 注: 远程库名一般叫origin,但并非强制,你可以自己起名.例:git remote add online https://github.com/876205350/lianxi.git 修改远程库名称git remote rename &lt;旧名称&gt; &lt;新名称&gt;例:git remote rename online oschina 公钥登陆配置请看 ssh key配置 配置ssh格式的远程仓库地址git remote add origin git@github.com:876205350/lianxi.git push本地仓库到远程,发现不用填密码了git push -u origin master 未完待续…………]]></content>
      <categories>
        <category>git</category>
      </categories>
      <tags>
        <tag>Testing</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[laravel操作数据库]]></title>
    <url>%2F2018%2F05%2F08%2Flaravel%E6%93%8D%E4%BD%9C%E6%95%B0%E6%8D%AE%E5%BA%93%2F</url>
    <content type="text"><![CDATA[Laravel提供了3种操作数据库方式：DB facade（原始方式）、查询构造器和Eloquent ORM。 一、数据库操作之DB facade 查询方法：select，update，insert，delete，和statement。查询操作1234//返回二位数组DB::select('select * from users);DB::select('select * from users where active = ?', [1]);DB::select('select * from users where id = :id', ['id' =&gt; 1]); 新增操作12//返回true或falseDB::insert("insert into table(vip_ID,vip_name,vip_type,vip_fenshu) values(?,?,?,?)",[5,'小明','出行',670]); 更新操作12// 返回受影响行数DB::update('update vipinfo set vip_fenshu= ? where vip_ID= ? ',[700,5]); 删除操作12// 返回的是删除的行数。DB::delete('delete from vipinfo where vip_ID= ?',[5]); 运行一般性声明1DB::statement('drop table users'); 二 、数据库操作之查询构造器使用查询构造器实现查询123456789101112131415161718192021222324//get()返回多条数据 (对象)$student=DB::table("vipinfo")-&gt;get(); //first()返回1条数据 (对象)$student=DB::table("vipinfo")-&gt;first(); //结果集第一条记录 $student=DB::table("vipinfo")-&gt;orderBy('vip_ID','desc')-&gt;first();//按vip_ID倒序排序 //where()条件查询 $student=DB::table("vipinfo")-&gt;where('vip_ID','&gt;=',2)-&gt;get(); //一个条件 $student=DB::table("vipinfo")-&gt;whereRaw('vip_ID&gt; ? and vip_fenshu &gt;= ?',[2,300])-&gt;get(); //多个条件 dd($student); //pluck()指定字段,后面不加get $student=DB::table("vipinfo")-&gt;pluck('vip_name'); dd($student); //lists()指定字段，可以指定某个字段作为下标 $student=DB::table("vipinfo")-&gt;lists('vip_name','vip_ID'); //指定vip_ID为下标 dd($student); $student=DB::table("vipinfo")-&gt;lists('vip_name'); //不指定下标，默认下标从0开始 //select()指定某个字段 $student=DB::table("vipinfo")-&gt;select('vip_name','vip_ID')-&gt;get(); dd($student); //chunk()每次查n条 $student=DB::table("vipinfo")-&gt;chunk(2,function($students)&#123; //每次查2条 var_dump($students); if(.......) return false; //在满足某个条件下使用return就不会再往下查了 &#125;); 新增1234567891011121314151617181920212223242526$bool=DB::table("vipinfo")-&gt;insert(['vip_ID'=&gt;6,'vip_name'=&gt;'zls','vip_type'=&gt;"出行",'vip_fenshu'=&gt;800]); echo $bool; //返回bool值 //如果想得到新增的id，则使用insertGetId方法 $id=DB::table("vipinfo")-&gt;insertGetId(['vip_ID'=&gt;5,'vip_name'=&gt;'wyp','vip_type'=&gt;"出行",'vip_fenshu'=&gt;800]); echo $id; //插入多条数据 $bool=DB::table("vipinfo")-&gt;insert([ ['vip_ID'=&gt;5,'vip_name'=&gt;'wyp','vip_type'=&gt;"出行",'vip_fenshu'=&gt;800], ['vip_ID'=&gt;6,'vip_name'=&gt;'zls','vip_type'=&gt;"出行",'vip_fenshu'=&gt;800], ]); echo $bool; //返回bool值``` ### 修改``` bash$bool=DB::table("vipinfo")-&gt;where('vip_ID',6)-&gt;update(['vip_fenshu'=&gt;500]); echo $bool; //自增 $bool=DB::table("vipinfo")-&gt;where('vip_ID',6)-&gt;increment("vip_fenshu");//自增1 $bool=DB::table("vipinfo")-&gt;where('vip_ID',6)-&gt;increment("vip_fenshu",3);//自增3 echo $bool; //自减 $bool=DB::table("vipinfo")-&gt;where('vip_ID',6)-&gt;decrement("vip_fenshu");//自1 $bool=DB::table("vipinfo")-&gt;where('vip_ID',6)-&gt;decrement("vip_fenshu",3);//自增3 echo $bool; //自增时再修改其他字段 $bool=DB::table("vipinfo")-&gt;where('vip_ID',6)-&gt;increment("vip_fenshu",3,['vip_name'=&gt;'dbdibi']);//自增3 删除1234$num=DB::table("vipinfo")-&gt;where('vip_ID',6)-&gt;delete();//删除1条 $num=DB::table("vipinfo")-&gt;where('vip_ID','&gt;',4)-&gt;delete();//删除多条 echo $num; //删除的行数 $num=DB::table("vipinfo")-&gt;truncate();//删除整表，不能恢复，谨慎使用 使用聚合函数123456789101112//count()统计记录条数 $nums=DB::table("vipinfo")-&gt;count(); echo $nums; //max()某个字段的最大值,同理min是最小值 $max=DB::table("vipinfo")-&gt;max("vip_fenshu"); echo $max; //avg()某个字段的平均值 $avg=DB::table("vipinfo")-&gt;avg("vip_fenshu"); echo $avg; //sum()某个字段的和 $sum=DB::table("vipinfo")-&gt;sum("vip_fenshu"); echo $sum; 三、数据库操作之 - Eloquent ORM简介Laravel 内置的 Eloquent ORM 提供了一个美观、简单的与数据库打交道的 ActiveRecord 实现，每张数据表都对应一个与该表进行交互的模型（Model），通过模型类，你可以对数据表进行查询、插入、更新、删除等操作。在开始之前，确保在 config/database.php 文件中配置好了数据库连接。我们从创建一个 Eloquent 模型开始，模型类通常位于 app 目录下，你也可以将其放在其他可以被 composer.json 文件自动加载到的地方。所有 Eloquent 模型都继承自 Illuminate\Database\Eloquent\Model 类。 创建模型实例最简单的办法就是使用 Artisan 命令 make:model：1php artisan make:model User 如果你想要在生成模型时生成数据库迁移，可以使用 –migration 或 -m 选项：12php artisan make:model User --migrationphp artisan make:model User -m 建立模型，在app目录下建立一个Student模型，即Student.php，不需要带任何后缀。123456789&lt;?php namespace App; use Illuminate\Database\Eloquent\Model; class Student extends Model&#123; //指定表名 protected $table= &apos;vipinfo&apos;; //指定主键 protected $primaryKey= &apos;vip_ID&apos;; &#125; 在Student控制器里增加一个test3方法，配置路由 Route::get(‘test3’,[‘uses’=&gt;’StudentController@test3’]);1234567891011public function test3()&#123; // all()方法查询所有数据 $studnets=Student::all(); dd($studnets); //find()查询一条，依据主键查询。findOrFail()查找不存在的记录时会抛出异常 $student=Student::find(5); //主键为5的记录 var_dump($student[&apos;attributes&apos;]); //查询构造器的使用,省略了指定表名 $student=Student::get(); var_dump($student); &#125; 2 . 新增数据、自定义时间戳、批量赋值(1)使用save方法新增laravel会默认维护created_at,updated_at 两个字段，这两个字段都是存储时间戳，整型11位的，因此使用时需要在数据库添加这两个字段。如果不需要这个功能，只需要在模型里加一个属性：public $timestamps=false; 以及一个方法，可以将当前时间戳存到数据库123protected function getDateFormat()&#123; return time(); &#125; 控制器里写：1234567$student=new Student(); //设定数据 $student-&gt;vip_name=&apos;xiaoming&apos;; $student-&gt;vip_type=&apos;出行&apos;; $student-&gt;vip_fenshu=900; $bool=$student-&gt;save(); //保存 echo $bool; 从数据库里取得某条记录的时间戳时，默认取得的是按日期格式化好的时间戳，如果想取得原本的时间戳，则在模型里增加asDateTime方法。123protected function asDateTime($val)&#123; return $val; &#125; （2）使用create方法新增时，需要在模型里增加：1protected $fillable=[&apos;vip_name&apos;,&apos;vip_fenshu&apos;,&apos;vip_type&apos;]; //允许批量赋值的字段 控制器里写：1Student::create([&apos;vip_name&apos;=&gt;&apos;mmm&apos;,&apos;vip_fenshu&apos;=&gt;999,&apos;vip_type&apos;=&gt;&apos;出行&apos;]); （3）firstOrCreate()以属性查找记录，若没有则新增12$student=Student::firstOrCreate([&apos;vip_name&apos;=&gt;&apos;mmm&apos;]); echo $student; （4）firstOrNew()以属性查找记录，若没有则会创建新的实例。若需要保存，则自己调用save方法()123$student=Student::firstOrNew([&apos;vip_name&apos;=&gt;&apos;mmm&apos;]); $student-&gt;save(); echo $student; 修改数据123456//通过模型更新数据 $student=Student::find(2); $student-&gt;vip_fenshu=10000; $student-&gt;save(); //返回bool值 //通过查询构造器更新 $num=Student::where(&apos;vip_ID&apos;,&apos;&gt;&apos;,2)-&gt;update([&apos;vip_fenshu&apos;=&gt;2000]); echo $num; //返回更新的行数 删除数据12345678//(1)通过模型删除数据 $student=Student::find(11); $student-&gt;delete(); //返回bool值 //(2)通过主键删除 $num=Student::destroy(10); //删除主键为10的一条记录 echo $num; //返回删除的行数 $num=Student::destroy(10,5); //删除多条 或者$num=Student::destroy([10,5]); echo $num; //返回删除的行数]]></content>
      <categories>
        <category>laravel</category>
      </categories>
      <tags>
        <tag>Laravel</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[正则表达式]]></title>
    <url>%2F2017%2F09%2F11%2F%E6%AD%A3%E5%88%99%E8%A1%A8%E8%BE%BE%E5%BC%8F%2F</url>
    <content type="text"><![CDATA[字符串规则的表达式 初识规则· 具体字符· 字符边界· 字符集合[abcde],[0123456]· 字符补集[^adc]:不在abc范围内· 字符范围· 字符簇字符边界1234- ^ 匹配字符串的开始 - $ 匹配字符串的结尾 - \b 匹配单词的开始和结尾(边界) - \B 匹配单词的非边界 字符簇1234567891011+-------+---------------------+| 簇 | 含义 |+-------+---------------------+| . (点)| 任意字符，不含换行 || \w | [a-zA-Z0-9_] || \W | \w的补集 || \s | 空白符，包括\n\r\t\v等 || \S | 非空白符 || \d | [0-9] || \D | 非数字 |+-------+---------------------+ 单词匹配12345678910// 把字符串的hi单词找出来 $patt = '/\bhi\b/'; $str = 'hi, this is some history book'; preg_match_all($patt, $str, $res); print_r($res);// 包含在单词内hi找出来 $patt = '/\Bhi\B/'; $str = 'hi, this is some history book'; preg_match_all($patt, $str, $res); print_r($res); 集合与补集示例1234567891011/*给定手机号，必须由[01235689] 组成从字符串的开始找，找到字符结束^$ */ $patt = '/^[01235689]&#123;11&#125;$/';$patt = '/^[^47]&#123;11&#125;$/';$arr = array('13800138000','13426060134','170235','18289881234568782'); foreach($arr as $v) &#123; preg_match_all($patt, $v, $res); print_r($res); &#125; 字符范围123456// 找纯字母组成的单词$str = 'o2o, b2b, hello,world, that'; //$patt = '/\b[a-zA-Z]&#123;1,&#125;\b/'; //&#123;1,&#125;最少1个字母 $patt = '/\b[a-zA-Z]+\b/'; preg_match_all($patt, $str, $res); print_r($res); 字符簇就是系统规定好的表示方法12345678910// 把单词拆开$str = 'tommorw is another day , o2o , you dont bird me i dont bird you'; $patt = '/\W&#123;1,&#125;/'; // \W \w[a-zA-Z0-9]的补集//preg_split 通过正则表达式，分割字符串 print_r(preg_split($patt, $str)); //把多个空格或制表换成1个空格$str = 'a b heloo world'; // 'a b hello world'; $patt = '/\s&#123;1,&#125;/'; //\s空白字符，包括 \n\r\t\v等ᒵ //preg_replace — 执行正则的搜索和替换echo preg_replace($patt, ' ', $str); 匹配几个* 匹配前面的子表达式零次或多次。+ 匹配前面的子表达式一次或多次。\? 匹配前面的子表达式零次或一次。{n} n 是一个非负整数。匹配确定的 n 次。{n,m} m 和 n 均为非负整数，其中n &lt;= m 最少匹配 n 次且最多匹配 m 次。{n,} n 是一个非负整数。至少匹配n 次。12345678910// 5个字母组成单词 $patt = '/[a-zA-Z]&#123;5&#125;/'; // 3-5个字母组成单词 $patt = '/[a-zA-Z]&#123;3,5&#125;/'; // 5个以上字母组成单词 $patt = '/[a-zA-Z]&#123;5,&#125;/';$s = 'goooood,goood,goooooooooood'; $p = '/go+d/'; print_r(preg_replace($p,'god',$s)); 或者的用法12345678910// 查询纯数字或纯字母$str = 'hello o2o 2b9 250'; $patt = '/\b[a-zA-Z]+\b|\b[0-9]+\b/'; preg_match_all($patt, $str, $res); print_r($res);//查询苹果系列产品$str = 'ipad,iphone,imac,ipod,iamsorry'; $patt= '/\bi(pad|phone|mac|pod)\b/'; preg_match_all($patt, $str, $res); print_r($res); 贪婪与非贪婪12345678$str = 'ksda good goooood good kl s ja dfs dk ' // 把g（任意多内容）d 这样的字符串，换成god$patt = '/g.+d/'; // 默认贪婪模式（会尽量多的匹配）preg_match_all($patt, $str, $res); print_r($res); // god is not good $patt = '/g.+?d/'; //在数量(+ * &#123;n,&#125;)限定符后,加?,非贪婪模式preg_match_all($patt, $str, $res); print_r($res); // god , good 采集手机号12345$str = '王先生，联系13800138000,备用电话18902587413, QQ:258963,emai l:wang@qq.com, 身份证号: 1011011979112123036'// 采集电话号码$patt = '/\b1[358]\d&#123;9&#125;\b/'; preg_match_all($patt, $str, $res); print_r($res);]]></content>
      <categories>
        <category>PHP</category>
      </categories>
      <tags>
        <tag>正则</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[laravle 中 validate.php 汉化]]></title>
    <url>%2F2017%2F08%2F30%2Flaravel%E7%9A%84%E6%8A%A5%E9%94%99%E6%B1%89%E5%8C%96%2F</url>
    <content type="text"><![CDATA[错误提示汉化:1laravel 的验证机制很好用，但是默认提示文本是英文，相信很多小伙伴很苦恼吧。 位于\resources\lang\en\validation.php里面的提示全是英文 与en同级创建文件夹zh将en文件夹里内容粘贴到zh 将zh文件夹下的validation.php内容替换123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100&lt;?phpreturn [ 'unique' =&gt; ':attribute 已存在', 'accepted' =&gt; ':attribute 是被接受的', 'active_url' =&gt; ':attribute 必须是一个合法的 URL', 'after' =&gt; ':attribute 必须是 :date 之后的一个日期', 'alpha' =&gt; ':attribute 必须全部由字母字符构成。', 'alpha_dash' =&gt; ':attribute 必须全部由字母、数字、中划线或下划线字符构成', 'alpha_num' =&gt; ':attribute 必须全部由字母和数字构成', 'array' =&gt; ':attribute 必须是个数组', 'before' =&gt; ':attribute 必须是 :date 之前的一个日期', 'between' =&gt; [ 'numeric' =&gt; ':attribute 必须在 :min 到 :max 之间', 'file' =&gt; ':attribute 必须在 :min 到 :max KB之间', 'string' =&gt; ':attribute 必须在 :min 到 :max 个字符之间', 'array' =&gt; ':attribute 必须在 :min 到 :max 项之间', ], 'boolean' =&gt; ':attribute 字符必须是 true 或 false', 'confirmed' =&gt; ':attribute 二次确认不匹配', 'date' =&gt; ':attribute 必须是一个合法的日期', 'date_format' =&gt; ':attribute 与给定的格式 :format 不符合', 'different' =&gt; ':attribute 必须不同于:other', 'digits' =&gt; ':attribute 必须是 :digits 位', 'digits_between' =&gt; ':attribute 必须在 :min and :max 位之间', 'email' =&gt; ':attribute 必须是一个合法的电子邮件地址。', 'filled' =&gt; ':attribute 的字段是必填的', 'exists' =&gt; '选定的 :attribute 是无效的', 'image' =&gt; ':attribute 必须是一个图片 (jpeg, png, bmp 或者 gif)', 'in' =&gt; '选定的 :attribute 是无效的', 'integer' =&gt; ':attribute 必须是个整数', 'ip' =&gt; ':attribute 必须是一个合法的 IP 地址。', 'max' =&gt; [ 'numeric' =&gt; ':attribute 的最大长度为 :max 位', 'file' =&gt; ':attribute 的最大为 :max', 'string' =&gt; ':attribute 的最大长度为 :max 字符', 'array' =&gt; ':attribute 的最大个数为 :max 个', ], 'mimes' =&gt; ':attribute 的文件类型必须是:values', 'min' =&gt; [ 'numeric' =&gt; ':attribute 的最小长度为 :min 位', 'string' =&gt; ':attribute 的最小长度为 :min 字符', 'file' =&gt; ':attribute 大小至少为:min KB', 'array' =&gt; ':attribute 至少有 :min 项', ], 'not_in' =&gt; '选定的 :attribute 是无效的', 'numeric' =&gt; ':attribute 必须是数字', 'regex' =&gt; ':attribute 格式是无效的', 'required' =&gt; ':attribute 字段必须填写', 'required_if' =&gt; ':attribute 字段是必须的当 :other 是 :value', 'required_with' =&gt; ':attribute 字段是必须的当 :values 是存在的', 'required_with_all' =&gt; ':attribute 字段是必须的当 :values 是存在的', 'required_without' =&gt; ':attribute 字段是必须的当 :values 是不存在的', 'required_without_all' =&gt; ':attribute 字段是必须的当 没有一个 :values 是存在的', 'same' =&gt; ':attribute 和 :other 必须匹配', 'size' =&gt; [ 'numeric' =&gt; ':attribute 必须是 :size 位', 'file' =&gt; ':attribute 必须是 :size KB', 'string' =&gt; ':attribute 必须是 :size 个字符', 'array' =&gt; ':attribute 必须包括 :size 项', ], 'string' =&gt; ':attribute 必须是字符串', 'unique' =&gt; ':attribute 已经采取.', 'uploaded' =&gt; ':attribute 上传失败', 'url' =&gt; ':attribute 无效的格式', 'timezone' =&gt; ':attribute 必须个有效的时区', /* |-------------------------------------------------------------------------- | Custom Validation Language Lines |-------------------------------------------------------------------------- | | Here you may specify custom validation messages for attributes using the | convention "attribute.rule" to name the lines. This makes it quick to | specify a specific custom language line for a given attribute rule. | */ 'custom' =&gt; [ 'attribute-name' =&gt; [ 'rule-name' =&gt; 'custom-message', ], ], /* |-------------------------------------------------------------------------- | Custom Validation Attributes |-------------------------------------------------------------------------- | | The following language lines are used to swap attribute place-holders | with something more reader friendly such as E-Mail Address instead | of "email". This simply helps us make messages a little cleaner. | */ 'attributes' =&gt; [ 'username' =&gt; '用户名', 'account' =&gt; '账号', 'captcha' =&gt; '验证码', 'mobile' =&gt; '手机号', 'password' =&gt; '密码', 'content' =&gt; '内容', 'identity' =&gt; '手机号/用户名', ],]; 最后一步打开config/app.php 文件 将其中 ‘locale’ =&gt; ‘en’,修改为’locale’ =&gt; ‘zh’, 即可]]></content>
      <categories>
        <category>laravel</category>
      </categories>
      <tags>
        <tag>Laravel</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[mysql优化（六）sql语句分析]]></title>
    <url>%2F2017%2F06%2F06%2Fmysql%E4%BC%98%E5%8C%96%EF%BC%88%E5%85%AD%EF%BC%89sql%E8%AF%AD%E5%8F%A5%E5%88%86%E6%9E%90%2F</url>
    <content type="text"><![CDATA[如何利用explain来分析sql语句，利用OPTIMIZER_TRACE对排序语句分析。 一.sql语句的优化1.Sql语句的时间花在哪？等待时间和执行时间这两个时间并非孤立的，如果单条语句执行的快了对其他语句的锁定也就少了分析如何降低执行时间。 2.Sql语句的执行时间，又花到哪了？查找-&gt; 沿着索引查找慢着可能全表扫描。取出-&gt; 查找行后把结果取出来。 如何查询快？a) 查询的快—联合索引的顺序区分度，长度b) 取的快，索引覆盖c) 传输的少的行和列 切分查询：按数据拆成多次如 ： 插入10000行数据，每1000条为单位，插入分解查询：按照逻辑把多表链接查询分成多个简单查询sql， 3.sql语句优化思路？不查 —&gt; 少查 –&gt; 高效的查不查，通过业务逻辑来计算 比如 论坛注册会员的数，我们可以根据前三个月统计每天注册量，用程序估算。 少量，尽可能精准数据，我们观察新闻网站，内容等一般一次性取列表10 –30条左右 必须查的尽量走在索引查询行取时，取尽量少的列。比如 select * from tableA 取出所有列，不建议如果定量分析查的多少行和是否沿着索引查？ 二.使用explain来分析id:代表select语句的编号，如果是链接查询表之间是平等关系select编号是从1开始，如果某select中有子查询则编号递增。注意：id列数字越大越先执行，如果说数字一样大，那么就从上往下依次执行。12345678910111213141516171819202122232425262728mysql&gt; explain select id,user_name from l_manager where id in (select id from l_manager where id = 2)\G*************************** 1. row *************************** id: 1 select_type: SIMPLE table: l_manager partitions: NULL type: constpossible_keys: PRIMARY key: PRIMARY key_len: 4 ref: const rows: 1 filtered: 100.00 Extra: NULL*************************** 2. row *************************** id: 1 select_type: SIMPLE table: l_manager partitions: NULL type: constpossible_keys: PRIMARY key: PRIMARY key_len: 4 ref: const rows: 1 filtered: 100.00 Extra: Using index2 rows in set, 1 warning (0.00 sec) Select_type:查询类型 select_type Value JSON Name Meaning SIMPLE None Simple SELECT (not using UNION or subqueries) PRIMARY None Outermost SELECT UNION None Second or later SELECT statement in a UNION DEPENDENT UNION dependent (true) Second or later SELECT statement in a UNION, dependent on outer query UNION RESULT union_result Result of a UNION. SUBQUERY None First SELECT in subquery DEPENDENT SUBQUERY dependent (true) First SELECT in subquery, dependent on outer query DERIVED None Derived table SELECT (subquery in FROM clause) MATERIALIZED materialized_from_subquery Materialized subquery UNCACHEABLE SUBQUERY cacheable (false) A subquery for which the result cannot be cached and must be re-evaluated for each row of the outer query UNCACHEABLE UNION cacheable (false) The second or later select in a UNION that belongs to an uncacheable subquery (see UNCACHEABLE SUBQUERY) 各项内容含义说明： A：simple：表示不需要union操作或者不包含子查询的简单select查询。有连接查询时，外层的查询为simple，且只有一个。 B：primary：一个需要union操作或者含有子查询的select，位于最外层的单位查询的select_type即为primary。且只有一个。 C：union：union连接的select查询，除了第一个表外，第二个及以后的表select_type都是union。 D：dependent union：与union一样，出现在union 或union all语句中，但是这个查询要受到外部查询的影响 E：union result：包含union的结果集，在union和union all语句中,因为它不需要参与查询，所以id字段为null F：subquery：除了from字句中包含的子查询外，其他地方出现的子查询都可能是subquery G：dependent subquery：与dependent union类似，表示这个subquery的查询要受到外部表查询的影响 H：derived：from字句中出现的子查询。 I：materialized：被物化的子查询 J：UNCACHEABLE SUBQUERY：对于外层的主表，子查询不可被物化，每次都需要计算（耗时操作） K：UNCACHEABLE UNION：UNION操作中，内层的不可被物化的子查询（类似于UNCACHEABLE SUBQUERY） 1234 ----Simple(不含子查询)Select_type- ----Primary（含子查询或派生查询）----Derived （from型子查询） ----subquery(非from子查询) Table：查询针对的表有可能 实际表名或者表别名 （from A2 as a2）Derived 如from 型子查询Null 直接计算的结果不走表 Possible_key : 可能用到的索引注意：可能用几个索引最终只能用一个 Key ：最终用的索引Key_len : 使用索引最大长度注：有可能possible_keys为Null 而key不为NULL这种情况Possible_keys分析的是索引用于查找的过程。而最终的key可能是被用于查找，排序或索引覆盖 Type列： 是指查询的方式，非常重要是分析查数据的过程的重要依据可能的值如下：依次从好到差：system，const，eq_ref，ref，fulltext，ref_or_null，index_merge，unique_subquery，index_subquery，range，index，ALL，除了all之外，其他的type都可以使用到索引，除了index_merge之外，其他的type只可以用到一个索引 A：system：表中只有一行数据或者是空表，且只能用于myisam和memory表。如果是Innodb引擎表，type列在这个情况通常都是all或者index B：const：使用唯一索引或者主键，返回记录一定是1行记录的等值where条件时，通常type是const。其他数据库也叫做唯一索引扫描 C：eq_ref：出现在要连接过个表的查询计划中，驱动表只返回一行数据，且这行数据是第二个表的主键或者唯一索引，且必须为not null，唯一索引和主键是多列时，只有所有的列都用作比较时才会出现eq_ref D：ref：不像eq_ref那样要求连接顺序，也没有主键和唯一索引的要求，只要使用相等条件检索时就可能出现，常见与辅助索引的等值查找。或者多列主键、唯一索引中，使用第一个列之外的列作为等值查找也会出现，总之，返回数据不唯一的等值查找就可能出现。 E：fulltext：全文索引检索，要注意，全文索引的优先级很高，若全文索引和普通索引同时存在时，mysql不管代价，优先选择使用全文索引 F：ref_or_null：与ref方法类似，只是增加了null值的比较。实际用的不多。例如：SELECT * FROM ref_tableWHERE key_column=expr OR key_column IS NULL; G：index_merge：表示查询使用了两个以上的索引，最后取交集或者并集，常见and ，or的条件使用了不同的索引，官方排序这个在ref_or_null之后，但是实际上由于要读取所个索引，性能可能大部分时间都不如range H：unique_subquery：用于where中的in形式子查询，子查询返回不重复值唯一值 I：index_subquery：用于in形式子查询使用到了辅助索引或者in常数列表，子查询可能返回重复值，可以使用索引将子查询去重。 J：range：索引范围扫描，常见于使用 =, &lt;&gt;, &gt;, &gt;=, &lt;, &lt;=, IS NULL, &lt;=&gt;, BETWEEN, IN()或者like等运算符的查询中。 K：index：索引全表扫描，把索引从头到尾扫一遍，常见于使用索引列就可以处理不需要读取数据文件的查询、可以使用索引排序或者分组的查询。按照官方文档的说法： all ：意味着从标的第一行往后逐行扫描，运气不好扫描到最后一行。 通俗说：all扫描所有的数据行，相当于data_all index扫描所有索引节点相当于index_allRange：意思是查询时能根据索引做范围扫描1索引覆盖的查询情况下，能利用上索引数据，但利用不上索引查询必须全索引扫描 以上说的是索引扫描的两种情况，一种是查询使用了覆盖索引，那么它只需要扫描索引就可以获得数据，这个效率要比全表扫描要快，因为索引通常比数据表小，而且还能避免二次查询。在extra中显示Using index，反之，如果在索引上进行全表扫描，没有Using index的提示。 Ref：意思指通过索引列可以直接引用到某些数据行Eq_ref 是指通过索引列直接引用某1行数据常见于链接查询中Const，system，null 这三个分别指查询优化到常量级别甚至不需要查询时间一般按照主键查询易出现const，system或者直接查询某个表达式不经过表时，出现null Rows：预计要扫描多少行Extra：1234If you want to make your queries as fast as possible, look out for Extra column values of Using filesort and Using temporary,or, in JSON-formatted EXPLAINoutput, for using_filesort and using_temporary_table properties equal to true. 大概的意思就是说，如果你想要优化你的查询，那就要注意extra辅助信息中的using filesort和using temporary，这两项非常消耗性能，需要注意。 A：distinct：在select部分使用了distinc关键字。 B：no tables used：不带from字句的查询或者From dual查询。 C：使用not in()形式子查询或not exists运算符的连接查询，这种叫做反连接。即，一般连接查询是先查询内表，再查询外表，反连接就是先查询外表，再查询内表。 D：using filesort：排序时无法使用到索引时，就会出现这个。常见于order by和group by语句中。 E：using index：查询时不需要回表查询，直接通过索引就可以获取查询的数据。 F：using join buffer（block nested loop），using join buffer（batched key accss）：5.6.x之后的版本优化关联查询的BNL，BKA特性。主要是减少内表的循环数量以及比较顺序地扫描查询。 G：using sort_union，using_union，using intersect，using sort_intersection： using intersect：表示使用and的各个索引的条件时，该信息表示是从处理结果获取交集。 using union：表示使用or连接各个使用索引的条件时，该信息表示从处理结果获取并集。 using sort_union和using sort_intersection：与前面两个对应的类似，只是他们是出现在用and和or查询信息量大时，先查询主键，然后进行排序合并后，才能读取记录并返回。 H：using temporary：表示使用了临时表存储中间结果。临时表可以是内存临时表和磁盘临时表，执行计划中看不出来，需要查看status变量，used_tmp_table，used_tmp_disk_table才能看出来。 I：using where：表示存储引擎返回的记录并不是所有的都满足查询条件，需要在server层进行过滤。查询条件中分为限制条件和检查条件，5.6之前，存储引擎只能根据限制条件扫描数据并返回，然后server层根据检查条件进行过滤再返回真正符合查询的数据。5.6.x之后支持ICP特性，可以把检查条件也下推到存储引擎层，不符合检查条件和限制条件的数据，直接不读取，这样就大大减少了存储引擎扫描的记录数量。extra列显示using index condition。 J：firstmatch(tb_name)：5.6.x开始引入的优化子查询的新特性之一，常见于where字句含有in()类型的子查询。如果内表的数据量比较大，就可能出现这个。 K：loosescan(m..n)：5.6.x之后引入的优化子查询的新特性之一，在in()类型的子查询中，子查询返回的可能有重复记录时，就可能出现这个。 Index是指用到索引覆盖，效率非常高Using where 是指只靠索引无法定位。还需要where判断Using temporary 指用上了临时表，ground by 与 order by 不同列时或ground by 与 order by 别的表的列Using filesort 文件排序（文件可能在磁盘，也可能在内存） 三、OPTIMIZER_TRACE从MySQL5.6版本开始，optimizer_trace 可支持把MySQL查询执行计划树打印出来，对DBA深入分析SQL执行计划，COST成本都非常有用，打印的内部信息比较全面。默认是关闭的，功能支持动态开关，因为对性能有20%左右影响，只建议分析问题时，临时开启。 1. 默认是关闭的1mysql&gt; show variables like 'optimizer_trace'; Variable_name Value optimizer_trace enabled=off,one_line=off 1 row in set (0.05 sec) 2.演示 optimizer_trace 简单的使用流程：12345678910111213141516171819set tmp_table_size=1024;set sort_buffer_size=32768;set max_length_for_sort_data=16;set optimizer_trace_max_mem_size=1000000; --- 设置trace大小set end_markers_in_json=on; --- 增加trace中注释/* 打开 optimizer_trace，只对本线程有效 */SET optimizer_trace='enabled=on'; /* 执行语句 */select word from words order by rand() limit 3;/* 查看 OPTIMIZER_TRACE 输出 */SELECT * FROM `information_schema`.`OPTIMIZER_TRACE`\G/* 关闭 optimizer_trace*/SET optimizer_trace='enabled=off'; 补充：永久开启 optimizer_trace （重启失效）mysql&gt; set optimizer_trace="enabled=on"; 官方文档给出了一个不错的例子，比这里的这个要复杂多了，有兴趣的同学可自行翻阅：http://dev.mysql.com/doc/internals/en/tracing-example.html 四、SHOW PROCESSLISTSHOW PROCESSLIST显示哪些线程正在运行。您也可以使用mysqladmin processlist语句得到此信息。如果您有SUPER权限，您可以看到所有线程。否则，您只能看到您自己的线程MySQL保留一个额外的连接，让拥有SUPER权限的 账户使用，以确保管理员能够随时连接和检查系统（假设您没有把此权限给予所有的用户）。 12345678mysql&gt; show full processlist;+---------+-------------+--------------------+----------------+-------------+-------+-----------------------------------------------------------------------+-----------------------+| Id | User | Host | db | Command | Time | State | Info |+---------+-------------+--------------------+----------------+-------------+-------+-----------------------------------------------------------------------+-----------------------+| 1056536 | replication | 192.168.6.91:38417 | NULL | Binlog Dump | 33759 | Master has sent all binlog to slave; waiting for binlog to be updated | NULL || 1107067 | miaohr | 192.168.6.81:32024 | NULL | Query | 0 | NULL | show full processlist || 1107182 | miaohr | 192.168.6.91:44217 | hr_db_business | Sleep | 1 | | NULL |+---------+-------------+--------------------+----------------+-------------+-------+-----------------------------------------------------------------------+-----------------------+ 介绍一下八个参数： id #ID标识，要kill一个语句的时候很有用 use #当前连接用户 host #显示这个连接从哪个ip的哪个端口上发出 db #数据库名 command #连接状态，一般是休眠（sleep），查询（query），连接（connect） time #连接持续时间，单位是秒 state #显示当前sql语句的状态 info #显示这个sql语句 其中state的状态十分关键，下表列出state主要状态和描述： 状态 描述 Checking table 正在检查数据表（这是自动的）。 Closing tables 正在将表中修改的数据刷新到磁盘中，同时正在关闭已经用完的表。这是一个很快的操作，如果不是这样的话，就应该确认磁盘空间是否已经满了或者磁盘是否正处于重负中。 Connect Out 复制从服务器正在连接主服务器。 Copying to tmp table on disk 由于临时结果集大于tmp_table_size，正在将临时表从内存存储转为磁盘存储以此节省内存。 Creating tmp table 正在创建临时表以存放部分查询结果。 deleting from main table 服务器正在执行多表删除中的第一部分，刚删除第一个表。 deleting from reference tables 服务器正在执行多表删除中的第二部分，正在删除其他表的记录。 Flushing tables 正在执行FLUSH TABLES，等待其他线程关闭数据表。 Killed 发送了一个kill请求给某线程，那么这个线程将会检查kill标志位，同时会放弃下一个kill请求。MySQL会在每次的主循环中检查kill标志位，不过有些情况下该线程可能会过一小段才能死掉。如果该线程程被其他线程锁住了，那么kill请求会在锁释放时马上生效。 Locked 被其他查询锁住了。 Sending data 正在处理SELECT查询的记录，同时正在把结果发送给客户端。 Sorting for group 正在为GROUP BY做排序。 Sorting for order 正在为ORDER BY做排序。 Opening tables 这个过程应该会很快，除非受到其他因素的干扰。例如，在执ALTER TABLE或LOCK TABLE语句行完以前，数据表无法被其他线程打开。正尝试打开一个表。 Removing duplicates 正在执行一个SELECT DISTINCT方式的查询，但是MySQL无法在前一个阶段优化掉那些重复的记录。因此，MySQL需要再次去掉重复的记录，然后再把结果发送给客户端。 Reopen table 获得了对一个表的锁，但是必须在表结构修改之后才能获得这个锁。已经释放锁，关闭数据表，正尝试重新打开数据表。 Repair by sorting 修复指令正在排序以创建索引。 Repair with keycache 修复指令正在利用索引缓存一个一个地创建新索引。它会比Repair by sorting慢些。 Searching rows for update 正在讲符合条件的记录找出来以备更新。它必须在UPDATE要修改相关的记录之前就完成了。 Sleeping 正在等待客户端发送新请求. System lock 正在等待取得一个外部的系统锁。如果当前没有运行多个mysqld服务器同时请求同一个表，那么可以通过增加–skip-external-locking参数来禁止外部系统锁。 Upgrading lock INSERT DELAYED正在尝试取得一个锁表以插入新记录。 Updating 正在搜索匹配的记录，并且修改它们。 User Lock 正在等待GET_LOCK()。 Waiting for tables 该线程得到通知，数据表结构已经被修改了，需要重新打开数据表以取得新的结构。然后，为了能的重新打开数据表，必须等到所有其他线程关闭这个表。以下几种情况下会产生这个通知：FLUSH TABLES tbl_name, ALTER TABLE, RENAME TABLE, REPAIR TABLE, ANALYZE TABLE,或OPTIMIZE TABLE。 waiting for handler insert INSERT DELAYED已经处理完了所有待处理的插入操作，正在等待新的请求。 mysql 查看当前连接数命令： show processlist;如果是root帐号，你能看到所有用户的当前连接。如果是其它普通帐号，只能看到自己占用的连接。show processlist;只列出前100条，如果想全列出请使用show full processlist; MySQL 5.7版本，可以通过sys.innodb_lock_waits 表查到。1mysql&gt; select * from t sys.innodb_lock_waits where locked_table=`'test(数据库名)'.'t(表名)'`\G 可以看到，这个信息很全，4号线程是造成堵塞的罪魁祸首。而干掉这个罪魁祸首的方式，就是KILL QUERY 4或KILL 4。 不过，这里不应该显示“KILL QUERY 4”。这个命令表示停止4号线程当前正在执行的语句，而这个方法其实是没有用的。因为占有行锁的是update语句，这个语句已经是之前执行完成了的，现在执行KILL QUERY，无法让这个事务去掉id=1上的行锁。 实际上，KILL 4才有效，也就是说直接断开这个连接。这里隐含的一个逻辑就是，连接被断开的时候，会自动回滚这个连接里面正在执行的线程，也就释放了id=1上的行锁。 mysql&gt; show status;SHOW STATUS命令会显示每个服务器变量的名字和值，状态变量是只读的。我们可以在MySQL客户端下运行SHOW STATUS或者在命令行运用mysqladmin extended-status来查看这些变量。如果使用SQL命令，可以使用LIKE或者WHERE来限制结果。LIKE可以对变量名做标准模式匹配。(大概300多条数据 可以了解一下)123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294295296297298299300301302303304305306307308309310311312313314315316317318319320321322323324325326327328329330331332333334335336337338339340341342343344345346347348349350351352353354355356357+-----------------------------------------------+--------------------------------------------------+Variable_name | Value |+-----------------------------------------------+--------------------------------------------------+| Aborted_clients | 1 || Aborted_connects | 0 || Binlog_cache_disk_use | 0 || Binlog_cache_use | 0 || Binlog_stmt_cache_disk_use | 0 || Binlog_stmt_cache_use | 0 || Bytes_received | 1968 || Bytes_sent | 165729 || Com_admin_commands | 0 || Com_assign_to_keycache | 0 || Com_alter_db | 0 || Com_alter_db_upgrade | 0 || Com_alter_event | 0 || Com_alter_function | 0 || Com_alter_instance | 0 || Com_alter_procedure | 0 || Com_alter_server | 0 || Com_alter_table | 0 || Com_alter_tablespace | 0 || Com_alter_user | 0 || Com_analyze | 0 || Com_begin | 0 || Com_binlog | 0 || Com_call_procedure | 0 || Com_change_db | 1 || Com_change_master | 0 || Com_change_repl_filter | 0 || Com_check | 0 || Com_checksum | 0 || Com_commit | 0 || Com_create_db | 0 || Com_create_event | 0 || Com_create_function | 0 || Com_create_index | 0 || Com_create_procedure | 0 || Com_create_server | 0 || Com_create_table | 0 || Com_create_trigger | 0 || Com_create_udf | 0 || Com_create_user | 0 || Com_create_view | 0 || Com_dealloc_sql | 0 || Com_delete | 0 || Com_delete_multi | 0 || Com_do | 0 || Com_drop_db | 0 || Com_drop_event | 0 || Com_drop_function | 0 || Com_drop_index | 0 || Com_drop_procedure | 0 || Com_drop_server | 0 || Com_drop_table | 0 || Com_drop_trigger | 0 || Com_drop_user | 0 || Com_drop_view | 0 || Com_empty_query | 0 || Com_execute_sql | 0 || Com_explain_other | 0 || Com_flush | 0 || Com_get_diagnostics | 0 || Com_grant | 0 || Com_ha_close | 0 || Com_ha_open | 0 || Com_ha_read | 0 || Com_help | 0 || Com_insert | 0 || Com_insert_select | 0 || Com_install_plugin | 0 || Com_kill | 0 || Com_load | 0 || Com_lock_tables | 0 || Com_optimize | 0 || Com_preload_keys | 0 || Com_prepare_sql | 0 || Com_purge | 0 || Com_purge_before_date | 0 || Com_release_savepoint | 0 || Com_rename_table | 0 || Com_rename_user | 0 || Com_repair | 0 || Com_replace | 0 || Com_replace_select | 0 || Com_reset | 0 || Com_resignal | 0 || Com_revoke | 0 || Com_revoke_all | 0 || Com_rollback | 0 || Com_rollback_to_savepoint | 0 || Com_savepoint | 0 || Com_select | 20 || Com_set_option | 7 || Com_signal | 0 || Com_show_binlog_events | 0 || Com_show_binlogs | 0 || Com_show_charsets | 0 || Com_show_collations | 0 || Com_show_create_db | 0 || Com_show_create_event | 0 || Com_show_create_func | 0 || Com_show_create_proc | 0 || Com_show_create_table | 0 || Com_show_create_trigger | 0 || Com_show_databases | 1 || Com_show_engine_logs | 0 || Com_show_engine_mutex | 0 || Com_show_engine_status | 0 || Com_show_events | 0 || Com_show_errors | 0 || Com_show_fields | 0 || Com_show_function_code | 0 || Com_show_function_status | 0 || Com_show_grants | 0 || Com_show_keys | 0 || Com_show_master_status | 0 || Com_show_open_tables | 0 || Com_show_plugins | 0 || Com_show_privileges | 0 || Com_show_procedure_code | 0 || Com_show_procedure_status | 0 || Com_show_processlist | 1 || Com_show_profile | 0 || Com_show_profiles | 0 || Com_show_relaylog_events | 0 || Com_show_slave_hosts | 0 || Com_show_slave_status | 0 || Com_show_status | 2 || Com_show_storage_engines | 0 || Com_show_table_status | 0 || Com_show_tables | 1 || Com_show_triggers | 0 || Com_show_variables | 8 || Com_show_warnings | 0 || Com_show_create_user | 0 || Com_shutdown | 0 || Com_slave_start | 0 || Com_slave_stop | 0 || Com_group_replication_start | 0 || Com_group_replication_stop | 0 || Com_stmt_execute | 0 || Com_stmt_close | 0 || Com_stmt_fetch | 0 || Com_stmt_prepare | 0 || Com_stmt_reset | 0 || Com_stmt_send_long_data | 0 || Com_truncate | 0 || Com_uninstall_plugin | 0 || Com_unlock_tables | 0 || Com_update | 0 || Com_update_multi | 0 || Com_xa_commit | 0 || Com_xa_end | 0 || Com_xa_prepare | 0 || Com_xa_recover | 0 || Com_xa_rollback | 0 || Com_xa_start | 0 || Com_stmt_reprepare | 0 || Connection_errors_accept | 0 || Connection_errors_internal | 0 || Connection_errors_max_connections | 0 || Connection_errors_peer_address | 0 || Connection_errors_select | 0 || Connection_errors_tcpwrap | 0 || Connections | 4 || Created_tmp_disk_tables | 12 || Created_tmp_files | 5 || Created_tmp_tables | 16 || Delayed_errors | 0 || Delayed_insert_threads | 0 || Delayed_writes | 0 || Flush_commands | 1 || Handler_commit | 16 || Handler_delete | 0 || Handler_discover | 0 || Handler_external_lock | 257 || Handler_mrr_init | 0 || Handler_prepare | 0 || Handler_read_first | 23 || Handler_read_key | 30 || Handler_read_last | 0 || Handler_read_next | 2 || Handler_read_prev | 0 || Handler_read_rnd | 3 || Handler_read_rnd_next | 59074 || Handler_rollback | 0 || Handler_savepoint | 0 || Handler_savepoint_rollback | 0 || Handler_update | 0 || Handler_write | 473 || Innodb_buffer_pool_dump_status | Dumping of buffer pool not started || Innodb_buffer_pool_load_status | Buffer pool(s) load completed at 191017 11:10:21 || Innodb_buffer_pool_resize_status | || Innodb_buffer_pool_pages_data | 396 || Innodb_buffer_pool_bytes_data | 6488064 || Innodb_buffer_pool_pages_dirty | 0 || Innodb_buffer_pool_bytes_dirty | 0 || Innodb_buffer_pool_pages_flushed | 140 || Innodb_buffer_pool_pages_free | 7796 || Innodb_buffer_pool_pages_misc | 0 || Innodb_buffer_pool_pages_total | 8192 || Innodb_buffer_pool_read_ahead_rnd | 0 || Innodb_buffer_pool_read_ahead | 0 || Innodb_buffer_pool_read_ahead_evicted | 0 || Innodb_buffer_pool_read_requests | 6547 || Innodb_buffer_pool_reads | 340 || Innodb_buffer_pool_wait_free | 0 || Innodb_buffer_pool_write_requests | 1437 || Innodb_data_fsyncs | 7 || Innodb_data_pending_fsyncs | 0 || Innodb_data_pending_reads | 0 || Innodb_data_pending_writes | 0 || Innodb_data_read | 7492096 || Innodb_data_reads | 503 || Innodb_data_writes | 157 || Innodb_data_written | 2328576 || Innodb_dblwr_pages_written | 2 || Innodb_dblwr_writes | 1 || Innodb_log_waits | 0 || Innodb_log_write_requests | 0 || Innodb_log_writes | 2 || Innodb_os_log_fsyncs | 4 || Innodb_os_log_pending_fsyncs | 0 || Innodb_os_log_pending_writes | 0 || Innodb_os_log_written | 1024 || Innodb_page_size | 16384 || Innodb_pages_created | 57 || Innodb_pages_read | 339 || Innodb_pages_written | 140 || Innodb_row_lock_current_waits | 0 || Innodb_row_lock_time | 0 || Innodb_row_lock_time_avg | 0 || Innodb_row_lock_time_max | 0 || Innodb_row_lock_waits | 0 || Innodb_rows_deleted | 0 || Innodb_rows_inserted | 24098 || Innodb_rows_read | 54115 || Innodb_rows_updated | 0 || Innodb_num_open_files | 21 || Innodb_truncated_status_writes | 0 || Innodb_available_undo_logs | 128 || Key_blocks_not_flushed | 0 || Key_blocks_unused | 6695 || Key_blocks_used | 3 || Key_read_requests | 6 || Key_reads | 3 || Key_write_requests | 0 || Key_writes | 0 || Locked_connects | 0 || Max_execution_time_exceeded | 0 || Max_execution_time_set | 0 || Max_execution_time_set_failed | 0 || Max_used_connections | 1 || Max_used_connections_time | 2019-10-17 11:13:59 || Not_flushed_delayed_rows | 0 || Ongoing_anonymous_transaction_count | 0 || Open_files | 14 || Open_streams | 0 || Open_table_definitions | 106 || Open_tables | 102 || Opened_files | 139 || Opened_table_definitions | 106 || Opened_tables | 109 || Performance_schema_accounts_lost | 0 || Performance_schema_cond_classes_lost | 0 || Performance_schema_cond_instances_lost | 0 || Performance_schema_digest_lost | 0 || Performance_schema_file_classes_lost | 0 || Performance_schema_file_handles_lost | 0 || Performance_schema_file_instances_lost | 0 || Performance_schema_hosts_lost | 0 || Performance_schema_index_stat_lost | 0 || Performance_schema_locker_lost | 0 || Performance_schema_memory_classes_lost | 0 || Performance_schema_metadata_lock_lost | 0 || Performance_schema_mutex_classes_lost | 0 || Performance_schema_mutex_instances_lost | 0 || Performance_schema_nested_statement_lost | 0 || Performance_schema_prepared_statements_lost | 0 || Performance_schema_program_lost | 0 || Performance_schema_rwlock_classes_lost | 0 || Performance_schema_rwlock_instances_lost | 0 || Performance_schema_session_connect_attrs_lost | 0 || Performance_schema_socket_classes_lost | 0 || Performance_schema_socket_instances_lost | 0 || Performance_schema_stage_classes_lost | 0 || Performance_schema_statement_classes_lost | 0 || Performance_schema_table_handles_lost | 0 || Performance_schema_table_instances_lost | 0 || Performance_schema_table_lock_stat_lost | 0 || Performance_schema_thread_classes_lost | 0 || Performance_schema_thread_instances_lost | 0 || Performance_schema_users_lost | 0 || Prepared_stmt_count | 0 || Qcache_free_blocks | 1 || Qcache_free_memory | 1031872 || Qcache_hits | 0 || Qcache_inserts | 0 || Qcache_lowmem_prunes | 0 || Qcache_not_cached | 20 || Qcache_queries_in_cache | 0 || Qcache_total_blocks | 1 || Queries | 45 || Questions | 44 || Select_full_join | 0 || Select_full_range_join | 0 || Select_range | 0 || Select_range_check | 0 || Select_scan | 27 || Slave_open_temp_tables | 0 || Slow_launch_threads | 0 || Slow_queries | 0 || Sort_merge_passes | 0 || Sort_range | 0 || Sort_rows | 6 || Sort_scan | 2 || Ssl_accept_renegotiates | 0 || Ssl_accepts | 0 || Ssl_callback_cache_hits | 0 || Ssl_cipher | || Ssl_cipher_list | || Ssl_client_connects | 0 || Ssl_connect_renegotiates | 0 || Ssl_ctx_verify_depth | 0 || Ssl_ctx_verify_mode | 0 || Ssl_default_timeout | 0 || Ssl_finished_accepts | 0 || Ssl_finished_connects | 0 || Ssl_server_not_after | || Ssl_server_not_before | || Ssl_session_cache_hits | 0 || Ssl_session_cache_misses | 0 || Ssl_session_cache_mode | NONE || Ssl_session_cache_overflows | 0 || Ssl_session_cache_size | 0 || Ssl_session_cache_timeouts | 0 || Ssl_sessions_reused | 0 || Ssl_used_session_cache_entries | 0 || Ssl_verify_depth | 0 || Ssl_verify_mode | 0 || Ssl_version | || Table_locks_immediate | 108 || Table_locks_waited | 0 || Table_open_cache_hits | 20 || Table_open_cache_misses | 109 || Table_open_cache_overflows | 0 || Tc_log_max_pages_used | 0 || Tc_log_page_size | 0 || Tc_log_page_waits | 0 || Threads_cached | 0 || Threads_connected | 1 || Threads_created | 1 || Threads_running | 1 || Uptime | 30105 || Uptime_since_flush_status | 30105 |+-----------------------------------------------+--------------------------------------------------+ SHOW VARIABLES LIKE ‘%timeout%’123456789101112131415161718mysql&gt; SHOW VARIABLES LIKE '%timeout%';+-----------------------------+----------+| Variable_name | Value |+-----------------------------+----------+| connect_timeout | 10 || delayed_insert_timeout | 300 || have_statement_timeout | YES || innodb_flush_log_at_timeout | 1 || innodb_lock_wait_timeout | 50 || innodb_rollback_on_timeout | OFF || interactive_timeout | 28800 || lock_wait_timeout | 31536000 || net_read_timeout | 30 || net_write_timeout | 60 || rpl_stop_slave_timeout | 31536000 || slave_net_timeout | 60 || wait_timeout | 28800 |+-----------------------------+----------+ 参数名 描述 connect_timeout 在获取链接时，等待握手的超时时间，只在登录时有效，登录成功这个参数就不管事了。主要是为了防止网络不佳时应用重连导致连接数涨太快，一般默认即可。 delayed_insert_timeout 这是为MyISAM INSERT DELAY设计的超时参数，在INSERT DELAY中止前等待INSERT语句的时间。 innodb_lock_wait_timeout 描述很长，简而言之，就是事务遇到锁等待时的Query超时时间。跟死锁不一样，InnoDB一旦检测到死锁立刻就会回滚代价小的那个事务，锁等待是没有死锁的情况下一个事务持有另一个事务需要的锁资源，被回滚的肯定是请求锁的那个Query。 innodb_rollback_on_timeout 这个参数关闭或不存在的话遇到超时只回滚事务最后一个Query，打开的话事务遇到超时就回滚整个事务。This variable was added in MySQL 5.1.15. interactive_timeout/wait_timeout 一个持续SLEEP状态的线程多久被关闭。线程每次被使用都会被唤醒为activity状态，执行完Query后成为interactive状态，重新开始计时。wait_timeout不同在于只作用于TCP/IP和Socket链接的线程，意义是一样的。 net_read_timeout / net_write_timeout 这个参数只对TCP/IP链接有效，分别是数据库等待接收客户端发送网络包和发送网络包给客户端的超时时间，这是在Activity状态下的线程才有效的参数 slave_net_timeout 这是Slave判断主机是否挂掉的超时设置，在设定时间内依然没有获取到Master的回应就人为Master挂掉了 更多的show variables like xxx 详解mysql运行时参数https://blog.csdn.net/mooncarp/article/details/51787694]]></content>
      <categories>
        <category>mysql</category>
      </categories>
      <tags>
        <tag>mysql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[mysql优化（五）重复索引与冗余索引]]></title>
    <url>%2F2017%2F05%2F28%2Fmysql%E4%BC%98%E5%8C%96%EF%BC%88%E4%BA%94%EF%BC%89%E9%87%8D%E5%A4%8D%E7%B4%A2%E5%BC%95%E4%B8%8E%E5%86%97%E4%BD%99%E7%B4%A2%E5%BC%95%2F</url>
    <content type="text"><![CDATA[冗余索引（重点） 重复索引： 是指的在相同的列上按照相同的顺序创建的相同类型的索引，应该避免这样创建重复索引，发现以后也应该立即删除。重复索引没有任何帮助，只会增大索引文件，拖慢更新速度。1234567CREATE TABLE test( ID INT NOT NULL PRIMARY KEY, A INT NOT NULL, B INT NOT NULL, UNIQUE(ID), INDEX(ID),) ENGINE=InnoDB; 这段SQL创建了3个重复索引。通常并没有理由这么做。 冗余索引冗余索引是指两个索引覆盖的列有重叠，称之为冗余索引 比如文章和标签表12345678+-------+--------+---------+| id | artid | tag |+-------+--------+---------+| 1 | 1 | PHP || 2 | 1 | Linux || 3 | 2 | MySQL || 4 | 2 | Oracle |+-------+--------+---------+ 在实际使用中有两种查询Artid —&gt; 查询文章 —&gt; tagTag –&gt; 查询文章 —&gt; artid 列:Selet tag from table where artid =2Selet artid from table where tag=PHP 建立索引Alter table 表名 add index …….思路 1 :Artid -查— tag -&gt; index artid（artid）；Tag –查— artid -&gt; index tag(tag)Extra：using where这种建立两个索引的思路不够优化 优化思路 1 思路 2（联合索引） ：Artid —查—tag -&gt; index artid（artid,tag）；Tag –查—artid –&gt; index tag(tag,artid)Extra：using where，using index 索引覆盖 具体在不同的数据库引擎上测试结果如下列:如果在整数列上有一个索引，现在需要额外增加一个很长的VARCHAR列来扩展该索引，那性能可能会急剧下降。特别是有查询把这个索引当做覆盖索引，或者这是MyISAM表并且有很多范围查询（由于MyISAM的前缀压缩）的时候。有一个userinfo表。这个表有1000000行，对每个state_id值大概有20000条记录。在state_id列有一个索引对下面的查询有用，假设查询名为Q1：1SELECT count(*) FROM userinfo WHERE state_Id=5; 一个简单的测试表明该查询的执行速度大概是每秒115次(QPS)。还有一个相关查询需要检索几个列的值，而不是只统计行数，假设名为Q2：1SELECT state_id,city,address FROM userinfo WHERE state_id=5; 对于这个查询，测试结果QPS小于10。提升该查询性能的最简单办法就是扩展索引为(state_id,city,address)，让索引能覆盖查询：1ALTER TABLE userinfo DROP KEY state_id, ADD KEY state_id_2(state_id,city,address); 索引扩展后，Q2运行得更快了，但是Q1却变慢了。如果我们想让两个查询都变得更快，就需要两个索引，尽管这样一来原来的单列索引是冗余的了。图1显示这两个查询在不同索引策略下的详细结果，分别使用MyISAM和InnoDB存储引擎。注意到只有state_id_2索引时，InnoDB引擎上的查询Q1的性能下降并不明显，这是因为InnoDB没有使用索引压缩。有两个索引的缺点是索引成本更高。图2显示了想表中插入100万行数据所需要的时间。可以看到，表中的索引越多插入速度越慢。一般来说，增加新索引将会导致INSERT、UPDATE、DELETE等操作的速度变慢，特别是当新增索引后导致达到了内存瓶颈的时候。解决冗余索引和重复索引的方法很简单，删除这些索引就可以，但首先要做的是找出这样的索引。]]></content>
      <categories>
        <category>mysql</category>
      </categories>
      <tags>
        <tag>mysql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[mysql优化（四）理想的索引与排序]]></title>
    <url>%2F2017%2F05%2F20%2Fmysql%E4%BC%98%E5%8C%96%EF%BC%88%E5%9B%9B%EF%BC%89%E7%90%86%E6%83%B3%E7%8A%B6%E6%80%81%E7%B4%A2%E5%BC%95%E4%B8%8E%E6%8E%92%E5%BA%8F%2F</url>
    <content type="text"><![CDATA[理想的索引 索引与排序 理想的索引如何选择索引1 查询频繁 2 区分度高 3 长度小 （占用内存少）4 尽可能覆盖常用查询字段 列如：区分度：100万用户，性别基本上男、女各50万 区分度低索引长度直接影响索引文件的大小，影响增删改的速度，并间接影响查询速度（占用内存多） 针对列中的值，从左往右截取部分来建立索引1 截取的越短，重复度越高，区分度越小，索引效果越不好。2 截取的越长，重复度越低，区分度越高，索引效果越好，但带来的影响越大，增删改慢并且影响查询速度。所以我们要在 区分度 + 长度 两者取一个平衡。 1 截取不同长度并测试其区分度。对于一般的系统应用，区分度能够达到0.1索引的性能就可以接受 （一行索引对应十条数据） 2 对于左前缀不易区分的列，建立索引技巧左前缀不易区分比如网址都是以（http://www）开头列的前11个字符都是一样的不易区分有以下两种方法：(1) 把列内容倒过来存储并建立索引moc.udiab.www//:ptth这样左前缀区分大(2) 伪hash索引效果同样存储url hash列 12345678910111213create table t10( id int primary key, url char(60) not null default&apos;&apos;);insert into t10 values(1,&apos;http://www.baidu.com&apos;),(2,&apos;http://www.sina.com&apos;),(3,&apos;http://www.sohu.cn&apos;),(4,&apos;http://www.jiaguo.net&apos;),(5,&apos;http://www.go.cn&apos;),alert table t10 add urlcrc int unsigned not null 函数crc32(url)计算URL列存入urlcrc列 查询时：select * from t10 where urlcrc = crc32(‘http://www.baidu.com‘) 3 多列索引多列索引的考虑因素列的查询频率，列的区分度注意一定要结合实际业务场景以商城为例 goods表中的cat_id(栏目),brand_id(品牌)做多列索引从计算数据得出区分度Brand_id区分度更高但是从商城实际业务看，顾客一般先选大分类-&gt;小分类-&gt;品牌最终选择（1）index(cat_id,brand_id), （2）index(cat_id,shop_price)来建立索引（建立两个复合索引）1 ，2两个索引称为冗余索引甚至可以再加（3）index（cat_id，brand_id，shop_price）1 3放在一起就是重复索引 但（3）中的前2列和（1）中的前两列一样，再去掉（1）最终建立：Index（cat_id，shop_price）和index（cat_id，brand_id,shop_price）这两个复合索引 索引与排序排序可能发生2种情况1 对于索引覆盖直接在索引上查询，就是有序的，using index在innodb引擎中沿着索引字段排序查询，也是自然有序的，而对于mysisam引擎，如果按某索引字段排序如id 但取出的字段中有未索引字段，myisam做法不是索引-&gt;回行，索引-&gt;回行。而是先取出所有行，再进行排序。 2 先取出数据，形成临时表做filesort文件排序，但文件可能在磁盘上，也可能在内存中我们的争取的目标…..取出来的数据本身就是有序的,利用索引来排序比如表：goods商品（cat_id，shop_price）组成联合索引Where cat_id=N order by shop_price 可以利用索引排序Select goods_id，cat_id，shop_price from goods order by shop_price,//using where 按照shop_price索引取的结果，本身就是有序的 Select goods_id,cat_id,shop_price from goods order by click_count//using filesort 用到文件排序即取出的结果再次排序]]></content>
      <categories>
        <category>mysql</category>
      </categories>
      <tags>
        <tag>mysql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[mysql优化（三）Btree聚簇非聚簇]]></title>
    <url>%2F2017%2F05%2F16%2Fmysql%E4%BC%98%E5%8C%96%EF%BC%88%E4%B8%89%EF%BC%89Btree%E8%81%9A%E7%B0%87%E9%9D%9E%E8%81%9A%E7%B0%87%2F</url>
    <content type="text"><![CDATA[Myisam与innodb引擎聚簇非聚簇 Myisam与innodb引擎，索引文件的异同Myisam索引与数据关系是，索引与数据分离，每个索引都指向在磁盘上的位置，也就是说主索引和次索引都指向物理行（磁盘位置） 我们称之为非聚簇索引。 Innodb索引与数据的关系，直接在索引树中，直接存储行的数据，次索引指向对主键的引用我们称之为聚簇索引。 如图所示Myisam与innodb引擎存储索引对比 注意：innodb来说1 主键索引即存储索引值，又在叶子中存储行的数据 2 如果没有主键（primary key）则会unique key做主键 3 如果没有unique key系统生成一个内部的rowid做主键 4 像innodb中，主键索引结构中即存储主键值又存储行数据这种结构称之为”聚簇索引” 聚簇索引优势：根据主键查询条目比较少时，不用回行（数据就在主键节点下）劣势：如果碰到不规则数据插入时，造成频繁的树上叶节点分裂 聚簇索引的页分裂过程 二叉平衡树插入数据会打破树的平衡，需要重新排列 二叉排序树的平衡旋转图例1 LL：右单旋转 2 RR左单旋转 3 LR平衡旋转：先左后右 4 RL平衡旋转：先右后左 高性能索引策略对于innodb而言因为节点下有数据文件，因此节点的分裂将会比较慢对于innodb的主键尽量用整型，而且是递增的整型如果是无规律的数据将会产生页的分裂，影响速度 索引覆盖查询列要被所使用的索引覆盖。索引覆盖是指 如果查询的列恰好是索引的一部分那么查询只需要在索引文件上进行不需要回行到磁盘进行找数据这种查询速度非常快称之为‘索引覆盖’不是所有类型的索引都可以成为覆盖索引。覆盖索引必须要存储索引的列，而哈希索引、空间索引和全文索引等都不存储索引列的值，所以MySQL只能使用B-Tree索引做覆盖索引 优点：1.索引条目通常远小于数据行大小，只需要读取索引，则mysql会极大地减少数据访问量。 2.因为索引是按照列值顺序存储的，所以对于IO密集的范围查找会比随机从磁盘读取每一行数据的IO少很多。 3.一些存储引擎如myisam在内存中只缓存索引，数据则依赖于操作系统来缓存，因此要访问数据需要一次系统调用 4.innodb的聚簇索引，覆盖索引对innodb表特别有用。(innodb的二级索引在叶子节点中保存了行的主键值，所以如果二级主键能够覆盖查询，则可以避免对主键索引的二次查询) 几种优化场景：1.无WHERE条件的查询优化： 执行计划中，type 为ALL，表示进行了全表扫描 如何改进？优化措施很简单，就是对这个查询列建立索引。如下，1ALERT TABLE t1 ADD KEY(staff_id); 再看一下执行计划 12345678910111213explain select sql_no_cache count(staff_id) from t1\G*************************** 1. row *************************** id: 1select_type: SIMPLE table: t1 type: indexpossible_keys: NULL key: staff_idkey_len: 1 ref: NULL rows: 1023849 Extra: Using indexrow in set (0.00 sec) possible_key: NULL，说明没有WHERE条件时查询优化器无法通过索引检索数据，这里使用了索引的另外一个优点，即从索引中获取数据，减少了读取的数据块的数量。 无where条件的查询，可以通过索引来实现索引覆盖查询，但前提条件是，查询返回的字段数足够少，更不用说select *之类的了。毕竟，建立key length过长的索引，始终不是一件好事情。经过再次查询时间缩短0.13sec 2、二次检索优化1234567891011select sql_no_cache rental_date from t1 where inventory_id&lt;80000;……| 2005-08-23 15:08:00 || 2005-08-23 15:09:17 || 2005-08-23 15:10:42 || 2005-08-23 15:15:02 || 2005-08-23 15:15:19 || 2005-08-23 15:16:32 |+---------------------+79999 rows in set (0.13 sec) 执行计划12345678910111213explain select sql_no_cache rental_date from t1 where inventory_id&lt;80000\G*************************** 1. row *************************** id: 1 select_type: SIMPLE table: t1 type: rangepossible_keys: inventory_id key: inventory_id key_len: 3 ref: NULL rows: 153734 Extra: Using index condition1 row in set (0.00 sec) Extra：Using index condition 表示使用的索引方式为二级检索，即79999个书签值被用来进行回表查询。可想而知，还是会有一定的性能消耗的 尝试针对这个SQL建立联合索引，如下：1alter table t1 add key(inventory_id,rental_date); 执行计划：12345678910111213explain select sql_no_cache rental_date from t1 where inventory_id&lt;80000\G*************************** 1. row *************************** id: 1 select_type: SIMPLE table: t1 type: rangepossible_keys: inventory_id,inventory_id_2 key: inventory_id_2 key_len: 3 ref: NULL rows: 162884 Extra: Using index1 row in set (0.00 sec) Extra：Using index 表示没有会标查询的过程，实现了索引覆盖]]></content>
      <categories>
        <category>mysql</category>
      </categories>
      <tags>
        <tag>mysql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[mysql优化（二）索引]]></title>
    <url>%2F2017%2F05%2F10%2Fmysql%E4%BC%98%E5%8C%96%EF%BC%88%E4%BA%8C%EF%BC%89%E7%B4%A2%E5%BC%95%2F</url>
    <content type="text"><![CDATA[数据库索引优化 索引优化策略1索引类型1.1B-tree索引名叫Btree，大方面看都用的平衡树，但是具体是实现上，各引擎稍有不同比如严格的说INDB引擎使用的是T-treeMyisam，innodb默认使用B-Tree索引排好序的快速查找结构 1.2hash索引在Memory表中默认是hash索引，hash理论时间复杂度O(1) 既然hash查找如此高效，为什么不能用hash索引？1 hash函数计算后的结果是随机的，如果在磁盘上放置数据， 以主键ID为例，那么随着ID的增长 id对应的行在磁盘上随机放置 2 无法对范围查找进行优化 3 无法利用前缀索引，比如 在btree中，field列的值“hellopworld”，并加索引查询xx == helloword，自然可以利用索引，xx == hello 也可以利用索引（左前缀索引），因为hash（‘helloword’）和hash（‘hello’）两者关系仍为随机。 4 排序也无法优化 5 必须回行 也就是说通过索引拿到数据位置，必须回到表中取数据 2：btree索引的常见误区2.1 在where条件常用的列上都加上索引列：where cat_id=3 and price&gt;100;查询第三个栏目，100元以上的商品 Cat_id上或price上都加上索引 只能用上cat_id或price索引，因为独立的索引同时只能用上1个 联合索引（多个列看成整体值建立索引）2.2 在多列上建立索引后查询那个列索引都将发挥作用多列索引上索引发挥作用，需要满足左前缀要求以index（a，b，c）位例（注意和顺序有关）123456789| 语法 | 索引是否发挥作用 ||Where a=3 |是 只使用a列 ||Where a=3 and b=5 |是 使用a b 列 ||Where a=3 and b=5 and c=4 | 是 使用a b c 列 ||Where b=3 / where c=4 |否 ||Where a=3 and c=4 |a列能发挥索引，c不能 ||Where a=3 and b&gt;10 and c=7 |a能利用，b能利用，c不能利用||where a=3 and b like ‘XXX%’ and c=7|a能利用，b能利用，c不能利用||where a=3 and b like ‘%XXX’ |a能利用，b不能利用 | abc三个索引ab相连接，bc相连接，都通则都能用否则断开后面索引都不能 什么是左前缀（左边准确等于几，后面不知道可以） 看题:假设某个表有一个联合索引（C1,C2,C3,C4）以下--只能使用联合索引的C1,C2,C3部分 A where C1=x and C2=x and C4&gt;x and C3=x (C1,C2,C3可以用到联合索引，C4可以用一半) B where C1=x and C2=x and C4=x order by C3=x(C1,C2,C3可以用到联合索引) C where C1=x and C4=x ground by C3,C2(只用C1，若ground byC2,C3则索引为C1,C2,C3) D where C1=? and C5=? order by C2,C3(同上) E where C1=? and C2=? and C5=? order by C2,C3 （查找使用C1,C2 排序使用C3） 希望此题对大家有帮助]]></content>
      <categories>
        <category>mysql</category>
      </categories>
      <tags>
        <tag>mysql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[mysql优化（一）建表]]></title>
    <url>%2F2017%2F05%2F05%2Fmysql%E4%BC%98%E5%8C%96%EF%BC%88%E4%B8%80%EF%BC%89%E5%BB%BA%E8%A1%A8%2F</url>
    <content type="text"><![CDATA[数据库设计建表 表优化与类型选择1．定长与变长分离如 id int占4个字节，char(4)占据4个字符长度也是定长 即每个单元值占据的字节是固定的 核心且常用字段宜建成一张表 注：而varchar，text，blob变长字段适合单方一张表，用主键和核心表关联起来 2．常用字段和不常用字段分离结合网站具体业务分析，分析字段查询场景，查询频度低的字段，单拆分出来 3．在一对多需要关联统计的字段上添加冗余字段空间换时间统计模块下贴子数量如果使用jion in联合查询则短时间内存增加相乘关系列选择原则：字符类优先级 整形&gt;data, time&gt;enum,char&gt;varchar&gt;blob,text列特点分析： 整型：定长没有国家/地区之分,没有字符集的差异。Enum类型：存男和女 底层进行转化成数字。Text 和 blob 无法使用内存临时表（排序等操作只能在磁盘上进行 ） 够用就行，不要慷慨（smallint，varchar（N））原因：大的字段浪费内存，影响速度。 尽力避免使用NULL愿因：NULL 不利于索引]]></content>
      <categories>
        <category>mysql</category>
      </categories>
      <tags>
        <tag>mysql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[mysql数据库忘记密码]]></title>
    <url>%2F2017%2F05%2F01%2Fmysql%E6%95%B0%E6%8D%AE%E5%BA%93%E5%BF%98%E8%AE%B0%E5%AF%86%E7%A0%81%2F</url>
    <content type="text"><![CDATA[数据库忘记密码 11. 开始 --&gt;cmd--&gt; net stop mysql (停用MySQL服务 没启动的可以省略) 找到安装路径 MySQL Server 5.1下的my.ini 打开 my.ini 找到 [mysqld] 然后在下面加上 这句： skip_grant_tables （意思好像是启动MySQL服务的时候跳过权限表认证 ） 然后就启动数据库修改密码了 1 开始 --&gt; cmd --&gt; netstart mysql (启动MySQL服务)---&gt; mysql 回车 ( 如果成功，将出现MySQL提示符) 输入use mysql; （连接权限数据库）。 改密码： 1update user set password=password("要修改的密码")where user="root";（别忘了最后加分号）。 修改失败的话用下面的sql语句 注意5.7以上版本的 ：1updateuser setauthentication_string=password('要修改的密码') where user='root' ; 刷新权限（必须步骤）：flush privileges; 。 退出 quit。 将第3 步的 my.ini里的 skip_grant_tables 去掉（启动MySQL服务的时候不能让他跳过权限表认证 ） 重启MySQL ，再进入，使用用户名root和刚才设置的新密码就可以登录了。]]></content>
      <categories>
        <category>mysql</category>
      </categories>
      <tags>
        <tag>mysql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[mysql密码正确无法登陆]]></title>
    <url>%2F2017%2F03%2F07%2Fmysql-password%2F</url>
    <content type="text"><![CDATA[MySQL root密码正确，却怎么也无法从本地登录提示：11 ERROR 1045 (28000): Access denied for user 'root'@'localhost'(using password: YES) 后来发现mysql库中的user表缺少一个root指向host：localhost的数据项，只有一个root指向host：主机名的数据项，故怎么也无法利用root账户登录MySQL。 总结一点就是root账户缺失了访问localhost主机的账户信息，导致无法本地登录。 那有什么办法恢复root登录呢?11. cmd --&gt; net stop mysql (停用MySQL服务 没启动的可以省略) 找到安装路径 MySQL Server 5.1下的my.ini 打开 my.ini 找到 [mysqld] 然后在下面加上 这句： skip_grant_tables （意思好像是 启动MySQL服务的时候跳过权限表认证 ） 会发现这时无密码就可以登录mysql了。 当然我们还必须修复root账户丢失的数据项。 第一种是因为root账户初始的时候有3条记录，包含root对应localhost，hostname，127.0.0.1三条账户数据，我们可以updatehost为其他两项中一项为localhost即可。 第二种是直接insert一条记录，host为localhost即可 查看一下你的表中user字段用户名是不是root是不是与你连接数据库地用户名一致 总结一下：即使root的host包含了主机名，127.0.0.1那么依然是无法正常登录的，这里必须要有localhost的host才行。 查看数据库mysql表中的host字段： 发现user表host字段中没有localhost，但是我的理解是%代表所有的主机都能登录的，为什么localhost不能呢，同样的情况我在5.0.45版的mysql上面做实验就不会发生localhost无法登录，我当前用的是5.1.57版的，难道是版本的问题? 接下来的修改很明显了：12mysql&gt; updateuser set host='localhost' where user='root' and host='%';mysql&gt; flush privileges; OK，退出mysql，重启mysql就解决问题了]]></content>
      <categories>
        <category>mysql</category>
      </categories>
      <tags>
        <tag>mysql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hexo标签显示]]></title>
    <url>%2F2016%2F08%2F20%2Farticle-two%2F</url>
    <content type="text"><![CDATA[Next主题标签和分类显示 用命令 :hexo new 文章名字;新建的文章如下1234567---title: article-twodate: 2017-08-30 20:37:32type: ""categories: tags: --- 下面说一下categories是分类 tags是标签有两种形式:1tages: [标签1,标签2,...标签n] 12345tags: - 标签1 - 标签2 ... - 标签n 说一下显示问题:标签显示的时候会有大中小三种，标签的大小是根据不同文章中使用相同标签的次数决定的次数最多的标签显示时字体大加粗。 如下面的图]]></content>
      <categories>
        <category>Hexo</category>
      </categories>
      <tags>
        <tag>Hexo</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hexo总结]]></title>
    <url>%2F2016%2F08%2F13%2Farticl-one%2F</url>
    <content type="text"><![CDATA[之前布好的个人博客竟然出问题了，没办法又重新来了一遍同样的主题同样主题在此总结一下。 总体来说可以分为四步第一就是把需要安装的软件装好1.安装Node（作用）:用来生成静态页面然后用检查是否安装成功，按下Win和R，打开运行窗口：在新打开的窗口中输入cmd，敲击回车，打开命令行界面。输入:1node -v 显示版本信息说明安装成功 2.下载Git安装文件：安装完成同样我们来检查一下Git是不是安装正确了，打开命令行，输入：1git --version 显示版本信息说明安装成功 github账户的注册和配置打开Github官网，在下图的框中，分别输入自己的用户名，邮箱，密码一定要确认注册，否则无法使用gh-pages！ ##安装Hexo1.下载安装hexo1npm install hexo-cli -g 来试试安装成功没有，输入以下命令查看Hexo的版本信息：1hexo -v 如果出现类似内容说明安装成功啦！1234567891011hexo-cli: 1.0.3os: Windows_NT 10.0.14393 win32 x64http_parser: 2.7.0node: 6.10.2v8: 5.1.281.98uv: 1.9.1zlib: 1.2.11ares: 1.10.1-DEVicu: 58.2modules: 48openssl: 1.0.2k 文件夹里面新建Hexo1hexo init 我们在开始前先执行命令：1npm install 这条命令是用来安装依赖包的，具体安装内容可以在package.json文件里找到。安装好了之后会看到一大串的信息，这里就不贴出来了。现在我们可以看到blog目录下的文件结构是这样的：node_modules：是依赖包public：存放的是生成的页面scaffolds：命令生成文章等的模板source：用命令创建的各种文章themes：主题_config.yml：整个博客的配置db.json：source解析所得到的package.json：项目所需模块项目的配置信息 接下来就是看看主题样式了1hexo server 执行完以后会有：12INFO Start processingINFO Hexo is running at http://localhost:4000/. Press Ctrl+C to stop. 然后输入在页面地址栏输入：localhost:4000 ##接下来就是上传到github用编辑器打开你的项目，修改_config.yml文件的一些配置(冒号之后都是有一个半角空格的)：1234deploy: type: git repo: https://github.com/YourgithubName/YourgithubName.github.io.git branch: master 执行安装命令：1npm install hexo-deployer-git --save 之后执行上传到github的命令：123hexo cleanhexo generatehexo deploy 这次的总结就写到这里吧!!! ##2019年重新使用Hexo创建编辑完文章（报错）原因:1文章图片文件夹内含有一个视频文件（500MB）;执行过Hexo hexo generate 然后执行hexo deploy报错原因大概是上传内容超过100MB.2.我将视频删除重新执行hexo generate -d依然报错（我确定不是Git,Hexo问题）解决:把hexo下面的.deploy_git手动删除然后输入Hexo clean;重新执行:1npm install hexo-deployer-git --save 之后执行上传到github的命令：123hexo cleanhexo generatehexo deploy 问题解决 #附件12345hexo cleanhexo g == hexo generatehexo d == hexo deployhexo s == hexo serverhexo n == hexo new]]></content>
      <categories>
        <category>Hexo</category>
      </categories>
      <tags>
        <tag>Hexo</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[win10电源选项设置控制风扇转动]]></title>
    <url>%2F2016%2F08%2F10%2Fwin10%2F</url>
    <content type="text"><![CDATA[最新版win10 :电源选项里面没有节能和性能模式怎么办？ 首先找到控制面板（两种方法）：第一种： 第二种： 点击电源选项 到如下页面 你可能会发现只有平衡没有节能,点击左侧创建电源计划,然后如图操作修改计划名称和上面选择的对应就好了 选择节能点击更改计划,然后点击更改高级电源设置（c） 找到如图红色框部分全部改为被动然后就完工了。]]></content>
  </entry>
</search>